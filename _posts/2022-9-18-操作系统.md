### 进程、线程、协程区别🐋🐋🧊🌟🌟🌟🌟
- 进程是资源分配的基本单位，线程是独立运行和调度的基本单位。
- 创建线程的系统开销比进程小，切换的代价也比进程低。
- 同个进程的线程**共享进程的资源**，自己仅拥有一点必不可少，能保证独立运行的资源，线程的独立性比进程低。【TCB，程序计数器，一组寄存器和堆栈】
- 支持多处理机系统，**一个进程的多个线程**可以分配到多个处理机上，线程具有更好的并发性。
***
1.  进程是CPU资源分配的基本单位，线程是独立运行和独立调度的基本单位（CPU上真正运行的是线程）。
2.  进程拥有自己的资源空间，一个进程包含若干个线程，线程与CPU资源分配无关，多个线程共享同一进程内的资源。
3.  线程的调度与切换比进程快很多。
***
线程有时被称为轻量级进程( Lightweight Process, LWP),是程序执行流的最小单元。一个标准的线程由线程ID、当前指令指针(PC)、寄存器集合和堆栈组成。

协程是一种用户态的轻量级线程。协程拥有自己的寄存器上下文和栈。

协程不是被操作系统内核所管理的，而是完全由程序所控制的，即在用户态执行。 这样带来的好处是：性能有大幅度的提升，因为不会像线程切换那样消耗资源。

虽然一个线程内的多个协程可以切换但是这多个协程是串行执行的，某个时刻只能有一个线程在运行，没法利用CPU的多核能力。

[参考](https://daimajiaoliu.com/daima/60b96a703431c09)

- 对资源的管理和保护要求高，不限制开销和效率时，使用多进程。
- 要求效率高，频繁切换时，资源的保护管理要求不是很高时，使用多线程。

### 上下文切换
当多任务内核决定运行另外的任务时, 它保存正在运行任务的当前状态, 也就是CPU寄存器中的全部内容。这些内容被保存在任务自己的堆栈中, 入栈工作完成后就把下一个将要运行的任务的当前状况从该任务的栈中重新装入CPU寄存器, 并开始下一个任务的运行。

### 引起线程上下文切换的原因
- 当前正在执行的**任务完成**，系统的CPU正常调度下一个任务。
- 当前正在执行的任务遇到**I/O等阻塞**操作，调度器挂起此任务，继续调度下一个任务。
- 多个任务并发抢占锁资源，当前任务**没有抢到锁资源**，被调度器挂起，继续调度下一个任务。
- 用户的代码**挂起当前任务**，比如线程执行yield()方法，让出CPU。
- 硬件中断。

### 堆和栈的区别
[参考](https://www.zhihu.com/question/19729973)
堆（heap）：用于动态分配内存，位于BSS和栈中间的地址区域，由程序员申请分配和释放。堆是从低地址位向高地址位增长，采用链式存储结构。频繁的malloc/free造成内存空间的不连续，会产生碎片。（经常问如何解决内存碎片？）当申请堆空间时库函数是按照一定的算法搜索可用的足够大的空间，因此堆的效率比栈要低的多。

栈(stack)： 由编译器自动释放，用来存放函数的参数值、局部变量等。每当一个函数被调用时，该函数的返回类型和一些调用的信息被存放到栈中，这个被调用的函数再为它的自动变量和临时变量在栈上分配空间。每调用一个函数一个新的栈就会被使用。栈区是从高地址位向低地址位增长的，是一块连续的内存区域，最大容量是由系统预先定义好的，申请的栈空间超过这个界限时会提示溢出。

### 进程切换与线程切换的区别？
每个进程都有自己的虚拟地址空间，进程内的所有线程共享进程的虚拟地址空间。

进程切换与线程切换的一个最主要区别就在于进程切换涉及到**虚拟地址空间的切换**而线程切换则不会。因为每个进程都有自己的虚拟地址空间，而线程是共享所在进程的虚拟地址空间的，因此同一个进程中的线程进行线程切换时不涉及虚拟地址空间的转换。

那么显然每个进程都有自己的页表，那么当进程切换后页表也要进行切换，页表切换后TLB就失效了，cache失效导致命中率降低，那么虚拟地址转换为物理地址就会变慢，表现出来的就是程序运行会变慢，而线程切换则不会导致TLB失效，因为线程线程无需切换地址空间，因此我们通常说线程切换要比较进程切换块，原因就在这里。

### 进程的状态转换
![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/进程状态.png)

### 内存分段、分页

这两个技术都是**为了利用和管理好计算机的资源--内存**。在分段这个技术还没有出现之前，程序运行是需要从内存中分配出足够多的连续的内存，然后把整个程序装载进去。

问题：
1、地址空间不隔离
2、程序运行时候的地址不确定
3、内存使用率低下

分段：它把虚拟地址空间映射到了物理地址空间，并且你写的程序操作的是虚拟地址。

分页这个技术仍然是一种虚拟地址空间到物理地址空间映射的机制。但是，粒度更加的小了。单位不是整个程序，而是某个“页”，一段虚拟地址空间组成的某一页映射到一段物理地址空间组成的某一页。

分页这个技术，它的虚拟地址空间仍然是连续的，但是，每一页映射后的物理地址就不一定是连续的了。正是因为有了分页的概念，程序的换入换出就可以以页为单位了。

### 内存伙伴(Buddy)算法
是为了核心内存管理能够快速响应请求，尽可能地在提高内存利用率的同时减少内存碎片的一种算法。

在操作系统分配内存的过程中，一个内存块经常被分成两个大小相等的内存块，这两个大小相等的内存块就处于伙伴关系。它满足3个条件：**两个块具有相同大小；物理地址是连续的；从同一个大块中拆分出来。**

采用一位二进制数来表示它们的伙伴关系。当这个位为1，表示其中一块在使用；当这个位为0，表示两个页面块都空闲或者都在使用。

刚开始时，两个伙伴块都空闲，它们的伙伴位为0，如果其中一块被使用，异或后得1；如果另一块也被使用，异或后得0；如果前面一块回收了异或后得1；如果另一块也回收了异或后得0。

**缺点：**
一个系统中，对内存块的分配，大小是随机的，一片内存中仅一个小的内存块没有释放，旁边两个大的就不能合并。
如果所需内存大小不是2的幂次方，就会有部分页面浪费。比如原来是1024个块，申请了16个块，再申请600个块就申请不到了，因为已经被分割了。

### 用户态和内核态的区别🐋🌟🌟
[参考](https://blog.csdn.net/qq_27093465/article/details/106124309)
「当进程运行在内核空间时就处于内核态，而进程运行在用户空间时则处于用户态。」

在内核态下，进程运行在内核地址空间中，此时 CPU 可以执行任何指令。运行的代码也不受任何的限制，可以**自由地访问任何有效地址，也可以直接进行端口的访问**。

在用户态下，进程运行在用户地址空间中，被执行的代码要受到 CPU 的诸多检查，它们只能访问映射其地址空间的**页表项**中规定的在用户态下**可访问页面的虚拟地址**，且只能对任务状态段(TSS)中 I/O 许可位图(I/O Permission Bitmap)中规定的可访问端口进行直接访问。

用户态和内核态是操作系统的两种运行级别，两者最大的区别就是特权级不同。用户态拥有最低的特权级R3，内核态拥有较高的特权级R0。**运行在用户态的程序不能直接访问操作系统内核数据结构和程序。**

三种方式从用户态切换到内核态：**系统调用、异常、外设(硬件)中断**。
所有的系统资源管理都是在内核空间中完成的。比如读写磁盘文件，分配回收内存，从网络接口读写数据等等。

### 为什么要区分用户态和内核态？
操作系统将虚拟地址空间划分为两部分，一部分为内核空间，另一部分为用户空间。内核空间被所有进程共享。

在 CPU 的所有指令中，有些指令是非常危险的，如果错用，将导致系统崩溃，比如**清内存、设置时钟**等。如果允许所有的程序都可以使用这些指令，那么系统崩溃的概率将大大增加。

所以，CPU 将指令分为特权指令和非特权指令，对于那些危险的指令，只允许操作系统及其相关模块使用，普通应用程序只能使用那些不会造成灾难的指令。

### 从用户态是怎么切换到核心态的？
[参考](https://www.jianshu.com/p/b5142bba3224)
- 系统调用：用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作，而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。
- 异常：当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。
- 外围设备的中断：当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序。

### 系统调用是什么
[参考](https://bbs.huaweicloud.com/blogs/198164)

### 线程和线程池的理解

线程池解决的核心问题就是资源管理问题。

线程池是指提前创建若干个线程，当有任务需要处理时，线程池里的线程就会处理任务，处理完成后的线程并不会被销毁，而是继续等待下一个任务。由于创建和销毁线程都是消耗系统资源的，所以，当某个业务需要频繁进行线程的创建和销毁时，就可以考虑使用线程池来提高系统的性能。

线程池的作用：
- 降低资源消耗。通过重复利用已经创建的线程，能够降低线程**创建和销毁**造成的消耗。
- 提高响应速度。当任务到达时，任务可以不需要等待线程的创建就能立即执行。
- 提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。

一方面避免了处理任务时创建销毁线程开销的代价，另一方面避免了线程数量膨胀导致的过分调度问题，保证了对内核的充分利用。

### 线程间通信🐋
- wait/notify机制
- 管道通信就是使用java.io.PipedInputStream 和 java.io.PipedOutputStream进行通信

***
- 锁机制：包括互斥锁/量（mutex）、读写锁（reader-writer lock）、自旋锁（spin lock）、条件变量（condition）
	- 互斥锁/量（mutex）：提供了以排他方式防止数据结构被并发修改的方法。
	- 读写锁（reader-writer lock）：允许多个线程同时读共享数据，而对写操作是互斥的。
	- 自旋锁（spin lock）与互斥锁类似，都是为了保护共享资源。互斥锁是当资源被占用，申请者进入睡眠状态；而自旋锁则循环检测保持者是否已经释放锁。
	- 条件变量（condition）：可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。
- 信号量机制(Semaphore)
	- 无名线程信号量
	- 命名线程信号量
- 信号机制(Signal)：类似进程间的信号处理
- 屏障（barrier）：屏障允许每个线程等待，直到所有的合作线程都达到某一点，然后从该点继续执行。
线程间的通信目的主要是用于线程同步，所以线程没有像进程通信中的用于数据交换的通信机制

### 进程间通信🐋🌟🌟🌟
每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核。

- 管道：管道传输数据是单向的。管道这种通信方式效率低，不适合进程间频繁地交换数据。
- 消息队列：消息队列是保存在内核中的消息链表。消息队列**不适合比较大数据的传输**，消息队列通信过程中，存在**用户态与内核态之间的数据拷贝**开销。
- 共享内存：共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中。
- 信号量：用了共享内存通信方式，带来新的问题，那就是如果多个进程同时修改同一个共享内存，很有可能就冲突了。为了防止**多进程竞争**共享资源，而造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问。
- 信号：对于异常情况下的工作模式，就需要用「信号」的方式来通知进程。信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令）。信号是进程间通信机制中**唯一的异步通信机制**，因为可以在任何时候发送信号给某一进程。
- Socket：前面提到的管道、消息队列、共享内存、信号量和信号都是在同一台主机上进行进程间通信，那要想**跨网络与不同主机上的进程**之间通信，就需要 Socket 通信了。
***
1.  **管道/匿名管道(Pipes)** ：用于具有亲缘关系的父子进程间或者兄弟进程之间的通信。
2.  **有名管道(Names Pipes)** : 匿名管道由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道。有名管道严格遵循**先进先出(first in first out)**。有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。
3.  **信号(Signal)** ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生；
4.  **消息队列(Message Queuing)** ：消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识。管道和消息队列的通信数据都是先进先出的原则。与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是**消息队列存放在内核中，只有在内核重启**(即，操作系统重启)或者显式地删除一个消息队列时，该消息队列才会被真正的删除。消息队列可以实现**消息的随机查询**,消息不一定要以先进先出的次序读取,也可以按消息的**类型**读取.比 FIFO 更有优势。**消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。**
5.  **信号量(Semaphores)** ：信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于**进程间同步**。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。
6.  **共享内存(Shared memory)** ：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。
7.  **套接字(Sockets)** : 此方法主要用于在客户端和服务器之间通过网络进行通信。套接字是支持 TCP/IP 的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程

### 共享内存
[参考](https://blog.csdn.net/Yyingc/article/details/72942865)
不同进程之间共享的内存通常安排为同一段物理内存。进程可以将同一段共享内存连接到它们自己的地址空间中，所有进程都可以访问共享内存中的地址。

共享内存的生命周期随内核

缺点：没有提供同步的机制，这使得我们在使用共享内存进行进程间通信时，往往要借助其他的手段来进行进程间的同步工作。

### 消息队列、共享内存、命名管道的特点及使用场景
[参考](https://bbs.huaweicloud.com/blogs/319720)
无名管道：一种半双工的通信方式，只能在具有亲缘关系的进程间使用（父子进程）
优点：简单方便
缺点：
- 局限于单向通信
- 只能创建在它的进程以及其有亲缘关系的进程之间
- 缓冲区有限

有名管道：一种半双工的通信方式，它允许无亲缘关系进程间的通信
优点：可以实现任意关系的进程间的通信
缺点：
- 长期存于系统中，使用不当容易出错
- 缓冲区有限

信号量（Semaphore）：一个计数器，可以用来控制多个线程对共享资源的访问
- 优点：可以同步进程
- 缺点：信号量有限

信号（Signal）：一种比较复杂的通信方式，用于通知接收进程某个事件已经发生
消息队列（Message Queue）：是消息的链表，存放在内核中并由消息队列标识符标识
- 优点：可以实现任意进程间的通信，并通过**系统调用**函数来实现消息发送和接收之间的同步，**无需考虑同步问题**，方便
- 缺点：信息的复制需要额外消耗 CPU 的时间，不适宜于信息量大或操作频繁的场合

共享内存（Shared Memory）：映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问
- 优点：无须复制，快捷，信息量大
- 缺点：通信是通过将共享空间缓冲区直接附加到进程的虚拟地址空间中来实现的，因此进程间的读写操作的同步问题。利用内存缓冲区直接交换信息，内存的实体存在于计算机中，只能同一个计算机系统中的诸多进程共享，不方便网络通信

套接字（Socket）：可用于不同计算机间的进程通信

### 信号有哪些
[参考](https://blog.csdn.net/GangStudyIT/article/details/80551912)
输入`kill -l`

### 共享内存怎么实现？🌟
为了让不同进程之间进行通信，需要让不同进程共享相同的物理内存。

两个步骤：
1. 创建共享内存。通过函数shmget()从内存中获取一块共享内存区域，该函数返回值为共享内存的ID。
2. 映射共享内存。通过函数shmat()将上一步获取的共享内存映射到具体的内存空间。

**先创建共享内存，再将共享内存映射到每个进程的地址空间中。**

### mmap
[参考](https://www.cnblogs.com/huxiao-tee/p/4660352.html)
mmap是一种内存映射文件的方法，即将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。
实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用read,write等系统调用函数。相反，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。

### 操作系统怎么申请内存？
[参考](https://blog.csdn.net/weixin_36145588/article/details/78363836)
1. 每个进程都有独立的虚拟地址空间，进程访问的虚拟地址并不是真正的物理地址； 
2. 虚拟地址可通过每个进程上的页表(在每个进程的内核虚拟地址空间)与物理地址进行映射，获得真正物理地址； 
3. 如果虚拟地址对应物理地址不在**物理内存**中，则产生缺页中断，真正分配物理地址，同时更新进程的页表；如果此时物理内存已耗尽，则根据内存替换算法淘汰部分页面至物理磁盘中。 

当一个进程发生缺页中断的时候，进程会陷入内核态，执行以下操作： 
1. 检查要访问的虚拟地址是否合法 
2. 查找/分配一个物理页 
3. 填充物理页内容（读取磁盘，或者直接置0，或者啥也不干） 
4. 建立映射关系（虚拟地址到物理地址） 
重新执行发生缺页中断的那条指令 
如果第3步，需要读取磁盘，那么这次缺页中断就是majflt，否则就是minflt。 

从操作系统角度来看，进程分配内存有两种方式，分别由两个系统调用完成：brk和mmap（不考虑共享内存）。
1. malloc小于128k的内存，使用brk分配内存，将_edata往高地址推(只分配虚拟空间，不对应物理内存(因此没有初始化)，第一次读/写数据时，引起内核缺页中断，内核才分配对应的物理内存，然后虚拟地址空间建立映射关系)
2.  默认情况下，malloc函数分配内存，如果请求内存大于128K（可由M_MMAP_THRESHOLD选项调节），那就不是去推_edata指针了，而是利用mmap系统调用，从堆和栈的中间分配一块虚拟内存。
	**原因：** brk分配的内存需要等到高地址内存释放以后才能释放（例如，在B释放之前，A是不可能释放的，这就是内存碎片产生的原因），而mmap分配的内存可以单独释放。

### 缺页中断
硬性页缺失是指相关的页在页缺失发生时未被加载进内存的情况。这时操作系统需要：
1. 寻找到一个空闲的页。或者把另外一个使用中的页写到磁盘上（如果其在最后一次写入后发生了变化的话），并注销在MMU内的记录
2. 将数据读入被选定的页
3. 向MMU注册该页

### 内存空间

为了充分利用和管理系统内存资源，Linux采用虚拟内存管理技术，利用虚拟内存技术让每个进程都有4GB 互不干涉的虚拟地址空间。

进程初始化分配和操作的都是基于这个「虚拟地址」，只有当进程需要实际访问内存资源的时候**才会建立虚拟地址和物理地址的映射**，调入物理内存页。

不管是用户空间还是内核空间，使用的地址都是虚拟地址，当需进程要实际访问内存的时候，会由内核的「请求分页机制」产生「缺页异常」调入物理内存页。

一个程序的内存空间：
- 代码段： 只读，可共享;用来存放程序执行代码的一块内存区域。
- 数据段：储存已被初始化了的静态数据。
- BSS段：用来存放程序中未初始化的全局变量的一块内存区域。如果一个全局变量没有被初始化（或被初始化为0），那么他就存放在bss段；如果一个全局变量被初始化为非0，那么他就被存放在data段。
- 堆：用于存放进程运行中被动态分配的内存段，它的大小并不固定，可动态扩张或缩减。
- 栈：栈用于维护函数调用的上下文。

[虚拟内存参考，重点关注](https://zhuanlan.zhihu.com/p/367915663)

![](image/缺页中断.png)

### 中断的处理和恢复
中断是一种使CPU中止正在执行的程序而转去处理特殊事件的操作，这些引起中断的事件称为中断源，它们可能是来自外设的输入输出请求，也可能是计算机的一些异常事故或其它内部原因。

[中断与中断处理过程](https://www.cnblogs.com/jdksummer/articles/2687265.html)
[中断及中断处理程序](https://blog.csdn.net/qq_28877125/article/details/72783433)

中断向量表每项的四个字节存放着该项对应中断的中断处理程序的入口地址。
软中断被响应后，CPU进入中断响应周期。CPU将中断类型号乘以4，得到中断向量表的入口地址，并执行一下动作：
（1）将标志寄存器压入堆栈。
（2）用清中断标志（IF）和单步标志（TF）禁止硬件中断，即关中断。
（3）将当前代码**段寄存器**的内容（CS）压栈。
（4）将当前**指令指针**（IP）压栈。
（5）转向中断服务程序入口并将控制交给中断服务程序。
在中断服务程序执行完后，即CPU接收到IRET指令时，它又将产生以下步骤：弹出IP和CS，恢复标志寄存器。

### 程序编译的过程
- 预处理阶段：编译器将C程序的头文件编译进来，还有宏的替换。
- 编译：这个阶段编译器主要做词法分析、语法分析、语义分析等，在检查无错误后后，把代码翻译成汇编语言。
- 汇编阶段得到机器语言
- 链接：printf函数存在于一个名为printf.o的单独预编译目标文件中。必须得将其并入到hello.o的程序中，链接器就是负责处理这两个的并入，结果得到hello文件，它就是一个可执行的目标文件。

### 逻辑地址和物理地址的转换
分页系统地址结构：前一部分为页号P，后一部分为偏移量W，即页内地址。

1. 根据逻辑地址计算出页号和页内偏移量
2. 判断页号是否越界
3. 查询页表，找到页号对应的页表项，确定页面
4. 用内存块号和页内偏移量得到物理地址
5. 访问目标内存单元

### 固态硬盘和机械硬盘的区别
机械硬盘内部有一个旋转的金属盘，上面有磁性涂层。数据就存储在这些盘片上，并通过安装在控制器上的磁头进行读取。这些磁头在相应的盘片上物理的寻找区域，这意味着，机械硬盘需要花费相当长的时间来保存和检索数据。

固态硬盘使用的是更简单的设计，它有一堆NAND闪存颗粒来存储数据。并且，与另一种流行的闪存存储技术即DRAM不同，NAND闪存颗粒是较为稳定的，这意味着NAND闪存即使在断电的情况下，也能保留电荷，从而保留数据。

### 匿名管道和命名管道如何生成？
[使用管道完成进程间通信](https://www.cnblogs.com/lfri/p/12696572.html)

### 线程同步有哪几种方式？
[参考](https://www.cnblogs.com/Terry-Wu/p/10788663.html)
- 有synchronized关键字修饰的方法
- 有synchronized关键字修饰的语句块
- 使用特殊域变量(volatile)实现线程同步
- ReentrantLock
- 使用ThreadLocal管理变量，则每一个使用该变量的线程都获得该变量的副本
- 使用LinkedBlockingQueue<\E>（阻塞队列）来实现线程的同步 

### 共享内存的原理
为了实现在多个进程间高效的数据通信，linux内核特地留下一块内存区，该内存区能够被需要的进程映射到自身的内存空间。因此，进程便能够直接对这块内存区进行读写操作。

1. 要使用共享内存，首先需要使用 shmget() 函数获取共享内存；
2. 需要调用 shmat() 函数把共享内存关联到某个虚拟内存地址上；

### 什么是死锁，死锁产生的条件🐋🌟🌟
死锁是指两个或多个进程在执行的过程中，因为**竞争资源而造成互相等待**的现象，若无外力作用，它们都无法推进下去。

1. 互斥：一个资源每次只能被一个进程使用。
2. 请求与保持：一个进程因请求资源而阻塞时，对已获得的资源保持不放。
3. 不剥夺：进程已获得的资源，在末使用完之前，不能强行剥夺。
4. 循环等待：若干进程之间形成一种头尾相接的循环等待资源关系。

### 解除死锁的方式🌟🌟🌟
[参考](https://zhuanlan.zhihu.com/p/61221667)
主要有三种方式：
-   死锁防止
-   死锁避免
-   死锁检测和恢复

**死锁防止：**
在程序运行之前防止发生死锁，死锁防止的策略就是至少破坏这四个条件其中一项。
破坏【请求与保持】：采用静态分配的方式，静态分配的方式是指进程必须**在执行之前就申请需要的全部资源**，且直至所要的资源全部得到满足后才开始执行。

破坏【不剥夺条件】：方法一：占有资源的进程若要申请新资源，必须主动释放已占有资源，若需要此资源，应该向系统重新申请。方法二：资源分配管理程序为进程分配新资源时，若有则分配；否则将剥夺此进程已占有的全部资源，并让进程进入等待资源状态，资源充足后再唤醒它重新申请所有所需资源。

破坏【循环等待条件】：给系统的所有资源编号，规定进程请求所需资源的顺序必须**按照资源的编号**依次进行。

**死锁避免：**
银行家算法：算法要做的是判断对请求的满足是否会进入**不安全状态**，如果是，就拒绝请求；否则予以分配。

**死锁检测和恢复：**
不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复。
判断**资源分配图**是否可以简化，不可以说明产生死锁。

死锁恢复：
- 资源剥夺法：剥夺陷于死锁的进程所占用的资源，但并不撤销此进程，直至死锁解除。
- 进程回退法：根据系统保存的检查点让**所有的进程回退**，直到足以解除死锁，这种措施要求系统建立保存检查点、回退及重启机制。
- 进程撤销法  
	- 撤销陷入死锁的所有进程，解除死锁，继续运行。
	- **逐个撤销陷入死锁的进程，回收其资源并重新分配，直至死锁解除。**
- 系统重启法：结束所有进程的执行并重新启动操作系统。这种方法很简单，但先前的工作全部作废，损失很大。

***
[参考](https://blog.csdn.net/qq_27068845/article/details/78818381)
**预防死锁**
- 资源一次性分配：破坏请求和保持条件。当某个资源只在进程结束时使用一小会，那么在进程运行期间，这个资源都被占用，资源利用率很低。比较好的方法是，进程开始时，只申请和使用进程启动的资源，在运行过程中不断申请新的资源，同时释放已经使用完的资源。
- 可剥夺资源：当进程新申请的资源不满足时，释放已经分配的资源。破坏不可剥夺条件。在使用某些资源，比如打印机时，当强制剥夺已分配资源的时候，会导致打印机资源打印的信息不连续的问题。
- 资源有序分配：系统给进程编号，按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。

**避免死锁**
银行家算法：分配资源前先评估风险，会不会在分配后导致死锁。即分配给一个进程资源的时候，该进程能否全部返还占用的资源。

**检测死锁**
建立资源分配表和进程等待表。

**解除死锁**
从其他进程强制剥夺资源给死锁进程。
可以直接撤销死锁进程，或撤销代价最小的进程。

### 银行家算法【死锁避免】具体实现
[参考例子](https://www.cnblogs.com/wkfvawl/p/11929508.html)
安全序列：序列中的每一个进程$P_i$到运行完成尚需的资源量不超过系统当前剩余的资源量与所有在序列中排在它前面的进程当前所占有的资源量之和。

银行家算法的实质就是要设法保证系统动态分配资源后不进入不安全状态，以避免可能产生的死锁。即没当进程提出资源请求且系统的资源能够满足该请求时，系统将判断满足此次资源请求后系统状态是否安全，如果判断结果为安全，则给该进程分配资源，否则不分配资源，申请资源的进程将阻塞。

![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/资源分配.png)

1. 判断当前时刻是否是安全状态：

找出小于available的need进程先执行，执行完后释放资源，available += max，然后继续判断其他进程。
max【进程需要的资源】 = allocation + need

2. 资源分配时，要同时满足：request小于need，request小于available
3. 修改available，need，allocation的值，继续做安全性检查，不安全就撤销分配，阻塞提出申请的进程


### 操作系统是怎么保证缓存一致性的？🧊
多个线程并发访问同一个共享变量的时候，这些线程的执行处理器上的高速缓存各自都会保留一份该共享变量的副本，这就带来一个问题一个处理器对其副本数据进行更新之后，其他处理器如何“察觉”到该更新并做出适当反应，以确保这些处理器后续读取该共享变量时能够读取到这个更新，这就是缓存一致性问题。

总线琐：当一个 CPU 核执行一个线程对其缓存中的数据进行操作的时候，它会向总线上发送一个 Lock 信号，此时其他的线程想要去请求主内存的时候，就会被阻塞，这样该处理器核心就可以独享这个共享内存。

缓存琐：如果某个内存区域数据，已经同时被两个或以上处理器核缓存，缓存锁就会通过缓存一致性机制阻止对其修改，以此来保证操作的原子性，当其他处理器核回写已经被锁定的缓存行的数据时会导致该缓存行无效。

**一致性协议MESI**
在 CPU 缓存中保存一个标记位，以此来标记四种状态：
1. Modified(更改过的，记为 M)：处于这一状态的数据，只在本 CPU 核中有缓存数据，而其他核中没有。同时其状态相对于内存中的值来说，是已经被修改的，只是没有更新到内存中。
2. Exclusive(独占的，记为 E)：处于这一状态的数据，只有在本 CPU 中有缓存，且其数据没有修改，即与内存中一致。
3. Shared(共享的，记为s)：处于这一状态的数据在多个 CPU 中都有缓存，且与内存一致。
4. Invalid(无效的，记为I)：本 CPU 中的这份缓存已经无效。

CPU的读取会遵循几个原则(其实就是上面说的嗅探)：
1.  一个处于 M 状态的缓存行，必须时刻监听所有试图读取**该缓存行对应的主存地址**的操作，如果监听到，则必须在此操作执行前把其缓存行中的数据写回 CPU。
2.  一个处于 S 状态的缓存行，必须时刻监听使该缓存行无效或者独享该缓存行的请求，如果监听到，则必须把其缓存行状态设置为 I。
3.  一个处于 E 状态的缓存行，必须时刻监听其他试图读取该缓存行对应的主存地址的操作，如果监听到，则必须把其缓存行状态设置为 S。  

当 CPU 需要读取数据时，如果其缓存行的状态是 I 的，则需要从内存中读取，并把自己状态变成 S，如果不是 I，则可以直接读取缓存中的值，但在此之前，必须要等待其他 CPU 的监听结果，如其他 CPU 也有该数据的缓存且状态是 M，则需要等待其把缓存更新到内存之后，再读取。

当 CPU 需要写数据时，只有在其缓存行是 M 或者 E 的时候才能执行，否则需要发出特殊的 RFO 指令(Read Or Ownership，这是一种总线事务)，通知其他 CPU 置缓存无效(I)，这种情况下性能开销是相对较大的。在写入完成后，修改其缓存状态为 M。

**简化思路：**
-   如果 CPU1 修改了某个共享变量的数据，需要广播给其他 CPU
-   缓存中没有这个数据的 CPU 直接丢弃这个广播消息，无需处理
-   缓存中有这个数据的 CPU 监听到广播后，将相应的 cache line 置为 invalid 状态
-   当这些 CPU 下次读取这个数据时发现缓存行失效就去内存读取


[缓存一致性问题和MESI协议](https://www.jianshu.com/p/26dc7a6ddd1f)
[缓存一致性协议硬核讲解](https://zhuanlan.zhihu.com/p/375706879)

### 缓存一致性协议
- M : modified
- E : exclusive
- S : shared
- I : invalid

- Local Read（LR）：当前 CPU 读操作
- Local Write（LW）：当前 CPU 写操作
- Remote Read（RR）：其他 CPU 读操作
- Remote Write（RW）：其他 CPU 写操作

如果严格按照 MESI 协议，会有严重的性能问题。所以为了解决上面两个问题，引入了**写缓冲区**（Store Buffer）和**失效队列**（Invalid Queue）

写缓冲区是属于每个 CPU 的，当使用了写缓冲区后，每当发生 LW，当前 CPU 不再阻塞地等待其他 CPU 的确认回执，而是直接将更新的值直接写入写缓冲区，然后继续执行后续指令。在进行 LR 时，CPU 会先在写缓冲区中查询记录是否存在，如果存在则会从写缓冲区中直接获取，这一机制即是 Store Fowarding。

失效队列也是属于每个 CPU，使用失效队列后，发生 RW 对应的 CPU 缓存不再同步地失效缓存并发送确认回执，而是将失效消息放入失效队列，立即发送确认回执。后续 CPU 会在空闲是对失效队列中的消息进行处理，将对应的 CPU 缓存失效。

- 写屏障（Store Memory Barrier）：告诉 CPU 在执行屏障之后的指令前，将所有在**存储缓存**（store buffer）中的数据同步到内存。
- 读屏障（Load Memory Barrier）：告诉 CPU 在执行任何的加载前，先处理所有在**失效队列**（Invalid）中的消息。

### [Linux同步IO和异步IO，阻塞IO和非阻塞IO](https://cloud.tencent.com/developer/article/1684951)
-   同步IO：导致请求进程阻塞，直到I/O操作完成。
-   异步IO：不导致请求进程阻塞。

阻塞IO：进程发起IO系统调用后，进程被阻塞，转到内核空间处理，整个IO处理完毕后返回进程。操作成功则进程获取到数据。

非阻塞IO：进程发起IO系统调用后，如果内核缓冲区没有数据，需要到IO设备中读取，进程返回一个错误而不会被阻塞；进程发起IO系统调用后，如果内核缓冲区有数据，内核就会把数据返回进程。

同步IO在数据拷贝到应用缓冲区期间进程阻塞，前四个都是同步IO。

![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/IO模型.png)

### [操作系统fork进程的具体流程](https://www.jianshu.com/p/484af1700176)
 一个进程调用fork（）函数后，系统先给新的进程分配资源，例如存储数据和代码的空间。然后把原来的进程的所有值都复制到新的新进程中，只有少数值与原来的进程的值不同。fork只拷贝下一个要执行的代码到新的进程。

在fork函数执行完毕后，如果创建新进程成功，则出现两个进程，一个是子进程，一个是父进程。在子进程中，fork函数返回0，在父进程中，fork返回新创建子进程的进程ID。我们可以通过fork返回的值来判断当前进程是子进程还是父进程。

### 父进程的文件描述符和子进程共享吗
通过fork（）创建子进程时，子进程继承父进程环境和上下文的大部分内容的拷贝，其中就包括文件描述符表。
（1）对于父进程在fork（）之前打开的文件来说，子进程都会继承，与父进程共享相同的文件偏移量。
（2）相反，如果父进程先进程fork，再打开my.dat，这时父子进程关于my.dat 的文件描述符表指向不同的系统文件表条目，也不再共享文件偏移量。

### 文件描述符具体是什么东西？
文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。

### [讲一讲CPU](https://juejin.cn/post/6999663181291323423)
计算机的构成元件中，根据程序的指令来进行数据运算，并控制整个计算机的设备称作CPU。CPU的内部由**寄存器、控制器、运算器和时钟**四个部分组成，各个部分之间由电流信号相互连通。

- **寄存器**可用来暂存指令数据等处理对象，可以将其看作内存的一种。一个CPU内部会有20~100个寄存器。
- **控制器**负责把内存上的指令、数据等读入寄存器，并根据指令的执行结果来控制整个计算机。
- **运算器**负责运算从内存读入寄存器的数据。
- **时钟**负责发出CPU开始计时的时钟信号。

### [讲一讲 Cache 的结构，以及 L1、L2、L3 Cache](https://nieyong.github.io/wiki_cpu/CPU%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84-Cache.html)
-   L1缓存分成两种，一种是指令缓存，一种是数据缓存。L2缓存和L3缓存不分指令和数据。在L1缓存中，有一个叫做Cache line的东西。 他表示cpu从一级缓存读取数据的最小单位。
-   L1和L2缓存在每一个CPU核中，L3则是所有CPU核心共享的内存。
-   L1、L2、L3的越离CPU近就越小，速度也就越快，越离CPU远，速度也越慢。

**cahce line是cache与主存之间进行数据交换的基本单位**。

**Cache映射方式**

直接映射：每个固定的cache line只能映射有共同特征的主存行，例如采用取模方式的映射方式。
全相联：
组相联：

![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/组相联.png)

1. 匹配组。蓝色的9bit选择某一个组（一共512个组，9bit表示完）；
2. 匹配路。匹配是上一步选定的组中的哪一路。在这里，有两种选择路的方式：index类型和hit类型。
3. 匹配tag头。将红色部分的虚拟地址（VA）转化成物理地址（PA），转化的物理地址和tag头中的匹配，那么就认为该地址处的值在Cache中存在，也就命中（hit）Cache。如果不匹配，那么就认为该指令在Cache中不存在，未命中（miss），此时就需要到内存中取指令。
4. 匹配指令位置。通过上面三步，已经找到Cache中的某一行了。但是一行中有16个字节，应该取那一个（或者连续几个）呢？黑色的4bit来确定。

### 知道 C、C++ 上 volatile 的实现吗？

### 内存屏障如何将缓存脏数据刷到主存的？
[volatile底层原理：从CPU架构到内存屏障之旅](https://juejin.cn/post/7070091066044579876)

1.  多个处理共用一个主内存，主内存访问速度太慢，于是引入高速缓存，以缓存行为单位。
2.  由于处理器在各自的高速缓存保存了一份共享变量的副本，产生了缓存一致性问题，于是在处理器层面引入了缓存一致性协议（Cache Coherence Protocol）。
3.  MESI 协议解决了缓存一致性问题，但自身存在性能缺陷：阻塞。
-   例如，如果 CPU0 发生 LW 时，首先会发 Invalidate 消息给到其他缓存了该数据的 CPU1，并且要等待 CPU1 的确认回执。CPU0 在这段时间内都会处于阻塞状态。
-   为了解决该问题，硬件设计者引入了写缓冲区Store Buffer 和失效队列 Invalidate Queue：
    -   **写缓冲区 Store Buffer**：每个处理器都有各自的写缓冲区。不用等待其他 CPU 的 Invalidate 确认回执，直接写入 Store Buffer，避免写阻塞。
    -   **失效队列 Invalidate Queue**：处理器收到 Invalidate 消息之后，并不立即 Invalidate Cache Line（这部操作相对较耗时），而是马上将消息入队，同时发送确认回执，之后再处理这些消息，和 Store Buffer 结合通过异步的方式减少了写操作的等待时间，解决了 CPU 阻塞的性能问题。
4.  写缓冲区和无效队列的引入又会带来新问题：内存重排序和可见性问题。
-   内存重排序：写缓冲区和失效队列都可能导致内存重排序。
    -   多线程场景下，内存重排序可能导致线程安全问题，造成线程间可见性问题。
5.  为了解决内存重排序和可见性问题，处理器引入内存屏障机制，提供内存屏障指令给软件层去调用
6.  以上是CPU 硬件层面，JVM 软件层面则是通过 **lock 前缀指令**来调用硬件层内存屏障指令
7.  在 Java 中可以通过对共享变量增加 volatile 修饰符来实现
-   JVM 会对 volatile 共享变量的读写操作添加 lock 前缀指令

### [CPU的原子操作](https://www.debugger.wiki/article/html/1607048280283786)
[参考](https://blog.csdn.net/qq_41184981/article/details/113832739)
586之前的CPU, 会通过LOCK锁总线的形式来实现原子操作。686开始则提供了存储一致性(Cache coherence),  这是多处理的基础, 也是原子操作的基础。

由于CPU对缓存的管理是以CacheLine为单位的, 所以在一个CacheLine内load/store实际上都是原子的。

“缓存锁定”指当发生共享内存的锁定，处理器不会在总线上声言LOCK#信号，而是修改内存地址，并通过缓存一致性机制保证原子性。

### [内存屏障分哪几种？](https://juejin.cn/post/7070091066044579876#heading-17)
1. LoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。【确保了 CPU1 在读该共享变量前先处理掉**失效队列**里的失效消息，及时将该共享变量置为失效（删除高速缓存中的副本），之后 CPU1才会重新去主内存读取最新的共享变量值，确保了可见性。】
2. StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。【StoreStore 屏障会将**写缓冲**区中现有的 Entry 做标记，表示这些 Entry的写操作要先于屏障之后的写操作被提交。】
3. LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷入主存前，保证Load1要读取的数据被读取完毕。
4. StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。【实际作用就是墙左边的写操作会被立即刷入主存，墙之后的读操作会从主存读最新的值。从而保证了可见性。】

volatile 具体是通过以下内存屏障策略，防止指令重排：

-   在 **volatile 写操作之前**，插入 **StoreStore 屏障**，保证当前 volatile 变量的写操作不会和之前的写操作重排序。（之前的写操作会先刷入主内存，这样 volatile 变量的写操作自然不会和前面重排序了）
-   在 **volatile 写操作之后**，插入 **StoreLoad 屏障**，保证当前 volatile 变量的写操作不会和之后的读操作冲排序。（确保当前的 volatile 变量写操作会先于后面的读操作被强刷入主内存）

-   在 volatile 读操作之后，插入 **LoadLoad 屏障、 LoadStore 屏障，** 保证当前 volatile 变量的读操作不会和之后的读、写操作重排序。

在Java 中是通过对共享变量增加 volatile 修饰符来插入内存屏障的：有了这个关键字，JVM 会在汇编层面会生层增加一个 lock 前缀指令来调用 CPU 层的内存屏障指令。

![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/JMM.png)

### [多级页表](https://blog.51cto.com/u_15352922/3742181)
优点：如果地址空间中大部分地址都没有使用，你不必为每一个index准备一个条目。

假定页大小是4K，32位地址总线进程地址空间大小为(2^32)4G，这时候页表项有 4G / 4K = 1048576个，每个页表项为一个地址，占用4字节，1048576 * 4(B) /1024(M) = 4M，也就是说一个程序啥都不干，页表大小就得占用4M。如果每个页表项都存在对应的映射地址那也就算了，但是，绝大部分程序仅仅使用了几个页，也就是说，只需要几个页的映射就可以了，如下图，进程1的页表，只用到了0,1,1024三个页，剩下的1048573页表项是空的，这就造成了巨大的浪费，为了避免内存浪费，计算机系统开发人员想出了一个方案，多级页表。

**书上的解释**
32位的虚拟地址空间有超过100万个页面，实际上只需要一个顶级页表，三个二级页表。

### 虚拟内存
当程序引用到一部分在物理内存中的地址空间时，由硬件执行必要的映射，当程序引用到一部分不在物理内存的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令。

在实际的硬件中，用一个“在/不在”记录页面【虚拟内存的页面】在内存【物理磁盘的页框】中的实际存在情况。

输入的16位虚拟地址被分为4位页号和12位偏移量。4位页号可以表示16个页面，12位偏移可以为一页内的全部4096个字节编址。最后形成的是单个物理地址。

![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/页表.jpg)

### 进程的状态🌟
[参考](https://blog.csdn.net/pange1991/article/details/53860651)
1. **创建状态(new)** ：进程正在被创建，尚未到就绪状态。
2. 就绪状态(ready) ：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。
3. 运行状态(running) ：进程正在处理器上上运行(单核CPU下任意时刻只有一个进程处于运行状态)。
4. **阻塞状态(waiting)** ：又称为等待状态，进程正在等待某一事件而暂停运行如等待某资源为可用或等待 IO 操作完成。即使处理器空闲，该进程也不能运行。
5. **结束状态(terminated)** ：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行。

### 线程状态
1. 初始(NEW)：新创建了一个线程对象，但还没有调用start()方法。
2. 运行(RUNNABLE)：Java线程中将就绪（ready）和运行中（running）两种状态笼统的称为“运行”。
线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获取CPU的使用权，此时处于就绪状态（ready）。就绪状态的线程在获得CPU时间片后变为运行中状态（running）。
3. 阻塞(BLOCKED)：表示线程阻塞于锁。
4. 等待(WAITING)：进入该状态的线程需要等待其他线程做出一些特定动作（通知或中断）。
5. 超时等待(TIMED_WAITING)：该状态不同于WAITING，它可以在指定的时间后自行返回。
6. 终止(TERMINATED)：表示该线程已经**执行完毕**。

### 进程调度算法
[参考](https://zhuanlan.zhihu.com/p/356089708)
- 先来先服务调度算法
- 最短作业优先调度算法
- 高响应比优先调度算法：(等待时间+要求服务时间)/要求服务时间
- 时间片轮转调度算法
- 最高优先级调度算法
- 多级反馈队列调度算法：设有N个队列（Q1,Q2....QN），其中各个队列对于处理机的优先级是不一样的。对于优先级最低的队列来说，里面是遵循**时间片轮转法**。也就是说，位于队列QN中有M个作业，它们的运行时间是通过QN这个队列所设定的时间片来确定的；对于其他队列，遵循的是先来先服务算法，每一进程分配一定的时间片，若时间片运行完时进程未结束，则进入下一优先级队列的末尾。各个队列的时间片是随着优先级的增加而减少的，也就是说，**优先级越高的队列中它的时间片就越短。**
***
[参考](https://blog.csdn.net/xy_cpp/article/details/79459237)
- 时间片轮转调度算法（RR）：给每个进程固定的执行时间，根据进程到达的先后顺序让进程在单位时间片内执行，执行完成后便调度下一个进程执行，时间片轮转调度不考虑进程等待时间和执行时间，属于抢占式调度。优点是兼顾长短作业；缺点是平均等待时间较长，上下文切换较费时。适用于分时系统。
- 先来先服务调度算法（FCFS）：根据进程到达的先后顺序执行进程，不考虑等待时间和执行时间，会产生饥饿现象。属于非抢占式调度，优点是公平，实现简单；缺点是不利于短作业。
- 优先级调度算法（HPF）：在进程等待队列中选择优先级最高的来执行。常被用于批处理系统中，还可用于实时系统中。
- 多级反馈队列调度算法：将时间片轮转与优先级调度相结合，把**进程按优先级分成不同的队列**，先按优先级调度，优先级相同的，**按时间片轮转**。优点是兼顾长短作业，有较好的响应时间，可行性强，适用于各种作业环境。
- 高响应比优先调度算法：根据“响应比=（进程执行时间+进程等待时间）/ 进程执行时间”这个公式得到的响应比来进行调度。高响应比优先算法在等待时间相同的情况下，作业执行的时间越短，响应比越高，满足段任务优先，同时响应比会随着等待时间增加而变大，优先级会提高，能够避免饥饿现象。优点是兼顾长短作业，缺点是计算响应比开销大，适用于批处理系统。

### 上下文切换的过程？🐋
[参考](https://cloud.tencent.com/developer/article/1710837)
进程上下文切换主要涉及到两部分主要过程：进程**地址空间**切换和**处理器状态**切换。

**进程地址空间切换：**
将即将执行的进程的**页全局目录的物理地址设置到页表基址寄存器**。地址空间切换过程中，还会**清空tlb**，防止当前进程虚拟地址转化过程中命中上一个进程的tlb表项。

**处理器状态（硬件上下文）切换：**
将前一个进程的sp,pc等寄存器的值保存到一块内存上，然后将即将执行的进程的sp,pc等寄存器的值从另一块内存中恢复到相应寄存器中，恢复sp完成了进程内核栈的切换，恢复pc完成了指令执行流的切换。

### 线程上下文切换
[参考](https://juejin.cn/post/6844903615367217165#heading-7)
上下文是指某一时间点 CPU 寄存器和程序计数器的内容。

- 保存进程A的状态（寄存器和操作系统数据）；
- 更新PCB中的信息，对进程A的“运行态”做出相应更改；
- 将进程A的PCB放入相关状态的队列；
- 将进程B的PCB信息改为“运行态”，并执行进程B；
- B执行完后，从队列中取出进程A的PCB，恢复进程A被切换时的上下文，继续执行A；

### PCB中有哪些信息
- 进程id。系统中每个进程有唯一的id，在c语言中用pid_t类型表示，其实就是一个非负整数。
- 进程的状态，有就绪，运行，挂起，停止等状态
- 进程切换时需要保存和恢复的一些CPU寄存器
- 描述虚拟地址空间的信息
- 描述控制终端的信息
- 当前工作目录（Current Working Directory）
- umask掩码
- 文件描述符表，包含很多指向结构体的指针
- 和信号相关的信息
- 用户id和组id。
- 会话（Session）和进程组
- 进程可以使用的资源上限（Resource Limit）

### 什么场景会导致线程的上下文切换？
![](image/上下文切换.png)
这种垃圾回收机制的使用有可能会导致 stop-the-world 事件的发生，这其实就是一种线程暂停行为。

### Thread.sleep(0) 有什么用？
操作系统中，CPU竞争有很多种策略。Unix系统使用的是时间片算法，而Windows则属于抢占式的。
所谓抢占式操作系统，就是说如果一个进程得到了 CPU 时间，除非它自己放弃使用 CPU ，否则将完全霸占 CPU 。
Thread.Sleep(0)的作用，就是“**触发操作系统立刻重新进行一次CPU竞争**”。竞争的结果也许是当前线程仍然获得CPU控制权，也许会换成别的线程获得CPU控制权。这也是我们在大循环里面经常会写一句Thread.Sleep(0) ，因为这样就给了其他线程比如Paint线程获得CPU控制权的权力，这样界面就不会假死在那里。

由于Java采用抢占式的线程调度算法，因此可能会出现某条线程常常获取到CPU控制权的情况，为了让某些优先级比较低的线程也能获取到CPU控制权，可以使用Thread.sleep(0)手动触发一次操作系统分配时间片的操作，这也是平衡CPU控制权的一种操作。

### 内存管理中的逻辑地址转换成物理地址的方法
[参考](https://www.cnblogs.com/felixfang/p/3420462.html)
每个操作系统都有自己的方法来保存页表。绝大多数都会为每个进程分配一个页表。现在由于页表都比较大，所以放在内存中(以往是放在一组专用寄存器里)，其指针存在进程控制块(PCB)里，当进程被调度程序选中投入运行时，系统将其页表指针从进程控制块中取出并送入用户寄存器中。随后可以根据此首地址访问页表。

![](image/虚拟地址.png)

### 调度方式
- 抢占式调度（Preemptive Scheduling）是一种CPU调度技术，它通过将CPU的时隙划分给给定的进程来工作。给定的时间间隔可能能够完成整个过程，也可能无法完成。当进程的区间时间（burst time）大于CPU周期时，它将被放回到就绪队列（ready queue）中，并在下一个时机（chance）执行。当进程切换到就绪状态时，会使用这种调度方式。抢占式调度支持的算法有循环调度(RR)、优先级（priority）调度、SRTF(剩余时间最短优先，shortest remaining time first)。
- 非抢占调度（Non-preemptive Scheduling）是一种CPU调度技术，进程获取资源(CPU时间)并持有它，直到进程终止或推送到等待状态。进程不会被中断，直到它完成，然后处理器切换到另一个进程。基于非抢占式调度的算法具有非抢占式优先（non-preemptive priority）以及最短作业优先级（shortest Job first）。

### 虚拟内存与物理内存
[参考](https://juejin.cn/post/6844903970981281800)
最初的操作系统并没有现在那么完善，刚开始的时候，程序是直接装载到物理内存中的。这就导致了下面的一些问题：

1. 程序编写困难。编写的时候就要考虑，自己的程序操作的内存地址，**是否已经被其他程序占用了**,如果被占用了的话，就要重新编写程序，重新安排程序的操作地址。
2. 修改内存数据导致程序崩溃。个别程序可以操作内存中的所有地址，如果有恶意程序修改了其他程序在用的地址中的数据，这就可能导致其他程序崩溃。

虚拟内存概念的出现就解决了上面的问题，用担心自己操作的内存地址被其他程序占用的问题了。也不会出现因为修改了其他应用程序内存地址中的数据而导致其他应用程序崩溃的问题了。

虚拟内存和物理内存都有分页的概念，一般一页是4096个字节，现在的操作系统虚拟地址和物理地址建立对应关系采用的是页映射的方式。

页表的每一个表项分为两部分，第一部分记录此页是否在物理内存上，第二部分记录物理内存页的地址(如果在的话)。当进程访问某个虚拟地址，就会先去看页表，如果发现对应的数据不在物理内存中，则发生缺页异常。

当程序执行发生页错误时，操作系统将**从物理内存中分配一个物理页，然后将该“缺页”从磁盘中读取到内存中，再设置缺页的虚拟页和物理页的映射关系**，这样程序才得以正常运行。

当操作系统装载一个可执行程序时，会首先创建**虚拟地址空间**，这个地址空间实际上就是一个数据结构；然后建立**可执行文件与虚拟地址的映射**，映射的作用就是知道虚拟空间对应可执行文件的哪个位置；最后就是**将CPU的指令寄存器设置成可执行文件的入口地址**，开始执行程序，程序开始执行的时候是会到入口地址那里找数据或程序执行，如果入口地址没有程序或数据，就会产生缺页中断，然后将虚拟地址对应可执行文件中的部分装载到物理内存中，再将这块物理内存和虚拟内存建立映射。

### 进程的PCB包含哪些信息
在进程控制块中，主要包括以下几个方面的信息：
- 进程标识符：进程标识符用于唯一的表示一个进程。一个进程通常有两种标识符：
	- 外部标识符。为了方便用户进程对进程的访问，须为每一个进程设置一个外部标识符。它是由创建者提供的，通常由字母、数字组成。为了描述进程的家族关系，还应设置父进程标识及子进程标识。此外，还可设置用户标识，以指示拥有该进程的用户。
	- 内部标识符。为了方便系统对进程的使用，在OS中又为进程设置了内部标识符，即赋予每一个进程一个唯一的数字标识符，它通常是一个**进程的序号**。

- 处理机状态：处理机状态信息也称为处理机的上下文，主要是由处理机的各种寄存器中的内容组成的。这些寄存器包括：
	- 通用寄存器，又称为用户可视寄存器，它们是用户程序可以访问的，用于暂存信息，在大多数处理机中，有8~32个通用寄存器，在RISC结构的计算机中可超过100个；
	- 指令计数器，其中存放了要访问的下一条指令的地址；
	- 程序状态字PSW，其中含有状态信息，如条件码、执行方式、中断屏蔽标志等；
	- 用户栈指针，指每个用户进程都有一个或若干个与之相关的系统栈，用于存放过程和系统调用参数及调用地址。栈指针指向该栈的栈顶。处理机处于执行状态时，正在处理的许多信息都是放在寄存器中。当进程被切换时，处理机状态信息都必须保存在相应的PCB中，以便在该进程重新执行是能再从断点继续执行。
- 进程调度信息：在OS进行调度时，必须了解进程的状态及有关进程调度的信息，这些信息包括：
	- 进程状态，指明进程的当前状态，它是作为进程调度和对换时的依据；
	- 进程优先级，是用于描述进程使用处理机的优先级别的一个整数，优先级高的进程应优先获得处理机；
	- 进程调度所需的其他信息，它们与所采用的进程调度算法有关，比如，进程已等待CPU的时间总和、进程已执行的时间总和等等；
	- 事件，是指进程由执行态转变为阻塞状态所等待发生的事件，即阻塞原因。
- 进程控制信息：是指用于进程控制所必须的信息，它包括：
	- 程序和数据的地址，进程实体中的程序和数据的内存或外存地址，以便再调度到该进程执行时，能从PCB中找到其程序和数据；
	- 进程同步和通信机制，这是实现同步和进程通信时必需的机制，如消息队列指针、信号量等，它们可能全部或部分地放在PCB中；
	- 资源清单，在该清单中列出了进程在运行期间所需的全部资源（除CPU），另外还有一张已分配到该进程的资源的清单；
	- 链接指针，它给出了本进程（PCB）所在队列中的下一个进程地PCB的首地址。

### 零拷贝
[参考](https://www.cnblogs.com/xiaolincoding/p/13719610.html)

![](image/mmap.png)

- 应用进程调用了 mmap() 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区；
- 应用进程再调用 write()，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；
- 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。

零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。

### 缓冲区溢出
[参考](https://www.jianshu.com/p/c95bf4d72f2b)
通过往程序的缓冲区写超出其长度的内容，造成缓冲区的溢出，从而破坏程序的堆栈，造成程序崩溃或使程序转而执行其它指令，以达到攻击的目的。造成缓冲区溢出的原因是程序中没有仔细检查用户输入的参数。
通常情况下，缓冲区溢出的数据只会破坏程序数据，造成意外终止。但是如果有人精心构造溢出数据的内容，那么就有可能获得系统的控制权！

- 淹没了其他的局部变量。如果被淹没的局部变量是条件变量，那么可能会改变函数原本的执行流程。这种方式可以用于破解简单的软件验证。
- 淹没了返回地址。这是栈溢出原理的核心所在，通过淹没的方式修改函数的返回地址，使程序代码执行“意外”的流程。
- 淹没参数变量。修改函数的参数变量也可能改变当前函数的执行结果和流程。

### 临界区🧊
临界资源：**一次仅允许一个进程使用的资源**。例如：物理设备中的打印机、输入机和进程之间共享的变量、数据。【一次仅允许一个进程所使用的资源叫做临界资源，比如打印机就是一个临界资源，只有当进程获取到打印机之后才能执行打印操作。】

临界区：每个进程中，**访问临界资源的那段代码**。【临界区（Critical Section）指的是一个访问共享资源的程序片段】

### 操作系统提供哪些锁？
- 互斥量：使用sleep和wakeup原语，保证同一时刻只有一个线程进入临界区代码片段的锁叫“互斥量”。
- 信号量：把互斥锁推广到"N"的空间，同时允许有N个线程进入临界区的锁叫“信号量”。互斥量和信号量的实现都依赖TSL指令保证“检查-占锁”动作的原子性。
- 管程：把互斥量交给程序员使用太危险，有些编程语言实现了“管程”的特性，从编译器的层面保证了临界区的互斥，比如Java的synchronized关键字。
- 自旋锁

### 段表和页表
[参考](https://www.cnblogs.com/xiaolincoding/p/13213699.html)
分段的问题：
- 内存碎片
- 内存交换的效率低【内存交换的空间太大】

在分页机制下，虚拟地址分为两部分，页号和页内偏移。页号作为页表的索引，页表包含物理页每页所在物理内存的基地址，这个基地址与页内偏移的组合就形成了物理内存地址。
![](image/二级页表.png)

多级页表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了几道转换的工序，这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。

我们就可以利用这一特性，把**最常访问的几个页表项**存储到访问速度更快的硬件，于是计算机科学家们，就在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，这个 Cache 就是 TLB（Translation Lookaside Buffer） ，通常称为页表缓存、转址旁路缓存、快表等。【MMU用来做地址转换】

![](image/段页表.png)

用于段页式地址变换的数据结构是每一个程序一张段表，每个段又建立一张页表，段表中的地址是页表的起始地址，而页表中的地址则为某页的物理页号。

### 阻塞和同步
阻塞，非阻塞：进程/线程要访问的数据是否就绪，进程/线程是否需要等待；
同步，异步：访问数据的方式，同步需要主动读写数据，在读写数据的过程中还是会阻塞；异步只需要I/O操作完成的通知，并不主动读写数据，由操作系统内核完成数据的读写。

***
- 当程序产生一个调用后，如果调用者**主动等待该调用的结果**，则称之为同步。
- 当程序产生一个调用后，如果调用者不以主动的方式等待结果，而是**被调用者完成任务后，通过某种形式（消息等...）通知调用者**，则称之为异步。

阻塞和非阻塞概念是以被调用者处理任务的时间内“**调用者的状态**”做区分。

同步方式：(1)阻塞：调用者什么都不干，就一直在等待，就是停在程序的某条执行语句上。(2)非阻塞：调用者去干别的活，但因为以主动等待结果的方式，所以得时不时的询问任务完成了没有。【主动轮询】

异步方式：(1)阻塞：调用者什么都不干，直至收到被调用者的通知。(2)非阻塞：调用者去干别的活，当被调用者完成任务后，通知调用者，调用者就可以处理被调用者的返回结果了。（回调就是用来处理此时返回的结果）

### 虚拟内存
[参考](https://zhuanlan.zhihu.com/p/82746153)
具有**请求调入和置换**功能，能从逻辑上对内存容量加以扩充的一种存储器系统。
系统允许将作业分多次调入内存，并能将内存中暂时不运行的程序和数据换至盘上，建立在离散分配的基础上。

通过虚拟地址访问内存有以下优势：
- 程序可以使用一系列相邻的虚拟地址来访问物理内存中不相邻的大内存缓冲区。
- 程序可以使用一系列虚拟地址来访问大于可用物理内存的内存缓冲区。当物理内存的供应量变小时，内存管理器会将物理内存页（通常大小为 4 KB）保存到磁盘文件。数据或代码页会根据需要在物理内存与磁盘之间移动。
- 不同进程使用的虚拟地址彼此隔离。一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存。

### 页面置换算法
- OPT页面置换算法（最佳页面置换算法） ：理想情况，不可能实现，一般作为衡量其他置换算法的方法。
- FIFO页面置换算法（先进先出页面置换算法） : 总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。
- LRU页面置换算法（最近未使用页面置换算法） ：LRU（Least Currently Used）算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间T，当须淘汰一个页面时，选择现有页面中其T值最大的，即最近最久未使用的页面予以淘汰。
- LFU页面置换算法（最少使用页面排序算法） : LFU（Least Frequently Used）算法会让系统维护一个按最近一次访问时间排序的页面链表，链表首节点是最近刚刚使用过的页面，链表尾节点是最久未使用的页面。访问内存时，找到相应页面，并把它移到链表之首。缺页时，置换链表尾节点的页面。也就是说内存内使用越频繁的页面，被保留的时间也相对越长。


### 段表和页表的区别
页表
页式管理的基本原理将各进程的虚拟空间划分成若干个长度相等的页(page)，页式管理把内存空间按页的大小划分成片或者页面（page frame），然后把页式虚拟地址与内存地址建立一一对应页表，并用相应的硬件地址变换机构，来解决离散地址变换问题。页式管理采用请求调页或预调页技术实现了内外存存储器的统一管理。

没有外碎片，每个内碎片不超过页大小。一个程序不必连续存放。便于改变程序占用空间的大小。

要求有相应的硬件支持。例如地址变换机构，缺页中断的产生和选择淘汰页面等都要求有相应的硬件支持。这增加了机器成本。增加了系统开销，例如缺页中断处理机，请求调页的算法如选择不当，有可能产生抖动现象。 虽然消除了碎片，但每个作业或进程的最后一页内总有一部分空间得不到利用果页面较大，则这一部分的损失仍然较大。

段表
把程序按内容或过程（函数）关系分成段，每段有自己的名字。
程序通过分段(segmentation)划分为多个模块，如代码段、数据段、共享段。其优点是： 可以分别编写和编译。 可以针对不同类型的段采取不同的保护。 可以按段为单位来进行共享，包括通过动态链接进行代码共享。

### 分段和分页的优缺点
- 内部碎片是指已分配但未被使用的地址空间。
- 外部碎片是指未分配且未使用的地址空间。

内部碎片是已被分配的空间，是操作系统不可利用的空间；外部碎片是未被分配的，是可分配的，但该空间过小(碎片的含义)无法装入资源，导致不可利用，但外部碎片是可解决的，可以将多个外部碎片紧凑成一个大的空闲空间，但这需要大量成本。

### 虚拟内存是什么？有什么优点？
[参考](https://www.jianshu.com/p/1e259231ce80)
因为**物理内存的空间是有限的**，进程所需的RAM数量总和远远超出了存储器所能支持的范围。为此引入了虚拟内存的思想。

每个程序**拥有自己的地址空间**，这个空间被**分割成多个块**，每一块称为一个页面。每一页有连续的地址范围。这些页**被映射到物理内存**，但并不是所有的页都必须在内存中才能运行程序。
当程序引用到一部分在物理内存中的地址空间时，由硬件立刻执行执行必要的映射。当程序引用到一部分不在物理内存的地址空间时，由操作系统负责将缺失的部分装入物理内存，并重新执行失败的指令。

在使用虚拟内存的情况下，虚拟地址不是被直接送到内存总线上，而是送到内存管理单元 --MMU。 MMU把虚拟地址映射到物理内存地址。

![](image/页表.png)

虚拟内存的优点：
- 进程与进程，进程与内核相互隔离
- 适当情况下，两个或更多的进程能共享内存
- 便于实现内存保护机制
- 驻留在内存中的仅是程序的一部分，所以程序的加载和运行都很快，而且一个进程所占用的内存能够超过RAM容量
- 由于每个进程使用的RAM减少了，RAM同时可以容纳的进程的数量就增多了，提高CPU的利用率

### 虚拟内存原理
[参考](https://www.cnblogs.com/cangqinglang/p/14754718.html)
虚拟内存是现代计算机中的一个非常重要的存储器抽象，主要是用来解决应用程序日益增长的内存使用需求：现代物理内存的容量增长已经非常快速了，然而还是跟不上应用程序对主存需求的增长速度，对于应用程序来说内存还是可能会不够用，因此便需要一种方法来解决这两者之间的容量差矛盾。为了更高效地管理内存并尽可能消除程序错误，现代计算机系统对物理主存 RAM 进行抽象，实现了虚拟内存 (Virtual Memory, VM)技术。

虚拟内存的核心原理是：为每个程序设置一段"连续"的虚拟地址空间，**把这个地址空间分割成多个具有连续地址范围的页 (Page)，并把这些页和物理内存做映射**，在程序运行期间动态映射到物理内存。当程序引用到一段在物理内存的地址空间时，由硬件立刻执行必要的映射；而当程序引用到一段不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令。

页表，每次进行虚拟地址到物理地址的映射之时，都需要读取页表，从数学角度来说页表就是一个函数，入参是虚拟页号，输出是物理页框号）。

使用虚拟内存技术的话，CPU 则是把这些虚拟地址通过地址总线送到内存管理单元（Memory Management Unit，简称 MMU），MMU 将虚拟地址翻译成物理地址之后再通过内存总线去访问物理内存。

加速地址翻译过程的方案目前是通过引入页表缓存模块 -- TLB【翻译后备缓冲器（Translation Lookaside Buffer，TLB）】，而大页表则是通过实现多级页表或倒排页表来解决。

前面我们学习到的页表设计都是以虚拟地址页号 VPN 作为页表项 PTE 索引，映射到物理页框号 PPN，而在倒排页表中则是以物理框号作为页表索引，映射到页号。

### 零拷贝
[参考](https://zhuanlan.zhihu.com/p/83398714)

**用户态直接 I/O** 只能适用于不需要内核缓冲区处理的应用程序，这些应用程序通常在进程地址空间有自己的数据缓存机制，称为自缓存应用程序，如数据库管理系统就是一个代表。其次，这种零拷贝机制会直接操作磁盘 I/O，由于 CPU 和磁盘 I/O 之间的执行时间差距，会造成大量资源的浪费，解决方案是配合异步 I/O 使用。
![](image/用户态.png)

一种零拷贝方式是使用 **mmap + write** 代替原来的 read + write 方式，减少了 1 次 CPU 拷贝操作。mmap 是 Linux 提供的一种内存映射文件方法，即将一个进程的地址空间中的一段**虚拟地址映射到磁盘文件地址**。

通过 sendfile 系统调用，数据可以直接在内核空间内部进行 I/O 传输，从而省去了数据在用户空间和内核空间之间的来回拷贝。与 mmap 内存映射方式不同的是， sendfile 调用中 I/O 数据对用户空间是完全不可见的。

在硬件的支持下，sendfile 拷贝方式不再从内核缓冲区的数据拷贝到 socket 缓冲区，取而代之的仅仅是缓冲区文件描述符和数据长度的拷贝，这样 DMA 引擎直接利用 gather 操作将页缓存中数据打包发送到网络中即可，本质就是和虚拟内存映射的思路类似。

**写时复制**指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么就需要将其拷贝到自己的进程地址空间中。这样做并不影响其他进程对这块数据的操作，每个进程要修改的时候才会进行拷贝，所以叫写时拷贝。