mysql的ACID（事务）
分布式的cap理论
cap中的c和acid中的c有区别吗
mysql的存储引擎了解的有哪些
数据库两种引擎，索引结构
数据库的引擎
innodb和MyISAM的区别
innodb中主键索引和非主键索引都是聚簇索引吗
事务回滚的实现
mysql主从架构
提问mysql双主架构在不分表的情况下保证数据一致性
如何避免mysql双主架构出现会循环的数据更新
外键的约束的作用
mysql事务隔离级别
不可重复读是什么
mysql索引(这里说了b+树，感觉面试官问的是唯一索引聚集索引那些，理解错意思x2)
创建索引的原则(区分度，是否是查询时使用，插入性能和读取性能权衡，减少回表)
MVCC
数据库隔离级别  
事务隔离级别
幻读举个例子  
innodb如何解决幻读
1.  慢SQL优化
2. 1.  水平分库分表场景
2.  IO密集型优化
3. 1.  MySQL索引
4. 1.  MySQL索引的理解（主键索引和二级索引）
5. 数据库的隔离级别
6. .读已提交，可重复读是什么意思
7. 数据库默认隔离级别
8. 分库分表会遇到什么问题？怎么解决？
9. 如果你发现查询很慢怎么办？explain
10. 关系型数据库和非关系型数据库
11. 数据库的锁
12. 数据库的隔离状态
7.       怎么防止幻读
数据库B+树
 Pagecache
 聚簇索性和非聚簇索性
 Sql语句查询前七天数据
 数据库索引
 - 聚集索引和非聚集索引
怎么实现的可重复读
(主要从锁机制+MVCC的角度讲，详细说明了当前要访问的版本的事务id和ReadView中活跃事务id列表的关系，如果大于最大值怎么样，小于最小值怎么样，介于最大值最小值之间要怎么办)

MySQL查询比较慢的话，通过什么方式来优化  
(情况1：偶尔很慢，可能是数据库在查询脏页，或者没拿到锁  
情况2：一直很慢，可能是没有索引，或者有索引但没走索引，或者表数据量太大需要分库分表)
mysql用过吗？讲一下b+树
[redis](https://www.nowcoder.com/jump/super-jump/word?word=redis)和mysql一致性解决方案

1.从联合索引出发，要求画出联合索引B+树索引图。

2.从mysql事务出发，要求画出RC和RR隔离级别下的MVCC。

3.从mysql update语句出发， 要求说出binlog 与 redolog一致性问题。

### 什么是视图，视图的使用场景有哪些？
视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。

- 重用SQL语句；
- 简化复杂的SQL操作；
- 使用表的组成部分而不是整个表；
- 授权使用视图，用户的到部分数据，这样做可以保护数据；
- 更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据；

### 数据库索引的缺点
- 索引会占据磁盘空间
- 索引虽然会提高查询效率，但是会降低更新表的效率。比如每次对表进行增删改操作，MySQL不仅要保存数据，还有保存或者更新对应的索引文件。

### [介绍下MySQL Innodb的索引数据结构](https://blog.csdn.net/qq_35246620/article/details/106221933)
B+树
为什么用B树而不用其他的如二叉树、平衡二叉树呢？
原因在于当存储关键字个数确定后，用B树进行存储所需要的**最大树高**相对于二叉树、平衡二叉树要小。这能够**减小IO次数**，从而提高检索效率。换句话说，使用B树能够确保，访问到任意数据的IO请求的最大次数是确定的。

InnoDB 存储引擎以页（默认为16KB）为基本单位存储：
1）非叶节点页用于存储键和键所在页的指针
2）叶节点（数据页）用于存储数据记录。

B+ 树也是多路平衡查找树，其与 B 树的区别主要在于：
- B 树中每个节点（包括叶节点和非叶节点）都存储真实的数据，B+ 树中只有叶子节点存储真实的数据，非叶节点只存储键。在 MySQL 中，这里所说的真实数据，可能是行的全部数据（如 InnoDB 的聚簇索引），也可能只是行的主键（如 InnoDB 的辅助索引），或者是行所在的地址（如 MyIsam 的非聚簇索引）。
- B 树中一条记录只会出现一次，不会重复出现，而 B+ 树的键则可能重复出现，一定会在叶节点出现，也可能在非叶节点重复出现。
- B+ 树的叶节点之间通过双向链表链接。
- B 树中的非叶节点，记录数比子节点个数少 1；而 B+ 树中记录数与子节点个数相同。

B+树比B树的优势：更少的 IO 次数、更适于范围查询、更稳定的查询效率

### InnoDB 和 MyISAM 的区别？🐋🐋🌟🌟
1. InnoDB 支持事务，MyISAM 不支持事务。这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；
2. InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败；
3. InnoDB 是聚集索引，MyISAM 是非聚集索引。聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。而 MyISAM 是非聚集索引，数据文件是分离的，**索引保存的是数据文件的指针**。主键索引和辅助索引是独立的。
4. InnoDB 不保存表的具体行数，执行 select count(\*) from table 时需要全表扫描。而MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快；
5. InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；

### 能细说一下 InnoDB 引擎吗？
InnoDB将数据划分为若干页，以页作为磁盘与内存交互的基本单位，一般页的大小为16KB。

[MySQL技术内幕：InnoDB存储引擎](https://segmentfault.com/a/1190000018129086)

### [单一业务ID有了解哪些算法吗](https://tech.meituan.com/2017/04/21/mt-leaf.html)

[参考](https://pdai.tech/md/arch/arch-z-id.html#defaultuidgenerator-%E5%AE%9E%E7%8E%B0)
UUID、snowflake和美团leaf算法
![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/snowflake.png)

Leaf-segment：利用proxy server批量获取，每次获取一个segment(step决定大小)号段的值。用完之后再去数据库获取新的号段，可以大大的减轻数据库的压力。
双buffer优化：采用双buffer的方式，Leaf服务内部有两个号段缓存区segment。当前号段已下发10%时，如果下一个号段未更新，则另启一个更新线程去更新下一个号段。当前号段全部下发完后，如果下个号段准备好了则切换到下个号段为当前segment接着下发，循环往复。

Leaf-snowflake方案：Leaf-segment方案可以生成趋势递增的ID，同时ID号是可计算的，不适用于订单ID生成场景。Leaf-snowflake方案完全沿用snowflake方案的bit位设计。使用Zookeeper持久顺序节点的特性自动对snowflake节点配置wokerID。

### snowflake有什么缺点？
强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态(如果单机时间出错导致业务ID重复)。

优点：
1. 生成ID时不依赖于DB，完全在内存生成，高性能高可用。
2. ID呈趋势递增，后续插入索引树的时候性能较好。

snowflake 受时间回拨影响的根本原因是高位采用时间戳的二进制值，而薄雾算法的高位是按序递增的数值。结果值的大小由高位决定，遂薄雾算法不受时间回拨影响。

### 讲讲什么时候使用表锁、什么时候使用行锁？
对于InnoDB表，在绝大部分情况下都应该使用行级锁，因为事务和行锁往往是我们之所以选择InnoDB表的理由。但在个另特殊事务中，也可以考虑使用表级锁。

第一种情况是：事务需要更**新大部分或全部数据**，表又比较大，如果使用默认的行锁，不仅这个事务执行效率低，而且可能造成其他事务长时间锁等待和锁冲突，这种情况下可以考虑使用表锁来提高该事务的执行速度。

第二种情况是：**事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚**。这种情况也可以考虑一次性锁定事务涉及的表，从而避免死锁、减少数据库因事务回滚带来的开销。

当然，应用中这两种事务不能太多，否则，就应该考虑使用ＭyISAＭ表。

对于**没有索引**的列，MySQL 会为这张表中所有行加行锁。在加上行锁后，MySQL 会进行一遍过滤，发现不满足的行就释放锁，最终只留下符合条件的行。


```sql
select * from information_schema.innodb_locks; # 锁的概况 
show engine innodb status; # InnoDB整体状态，其中包括锁的情况
```

其中lock_type为RECORD，代表锁为行锁(记录锁)；lock_mode为X，代表排它锁(写锁)。

### [关系型数据库(事务)的几个特性](https://www.cnblogs.com/kismetv/p/10331633.html)🌟
ACID
1. 原子性：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中**发生错误，会被回滚**（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。
2. 一致性：一致性是指事务执行结束后，数据库的**完整性约束没有被破坏**，事务执行的前后都是合法的数据状态。
3. 隔离性：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务**并发执行时由于交叉执行而导致数据的不一致**。
4. 持久性：**事务处理结束后，对数据的修改就是永久的**，即便系统故障也不会丢失。

例如MySQL的NDB Cluster事务不满足持久性和隔离性；InnoDB默认事务隔离级别是可重复读，不满足隔离性；Oracle默认的事务隔离级别为READ COMMITTED，不满足隔离性。

原子性实现：实现原子性的关键，是当事务回滚时能够撤销所有已经成功执行的sql语句。InnoDB实现回滚，靠的是undo log：当事务对数据库进行修改时，InnoDB会生成对应的undo log；如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。undo log属于**逻辑日志**，它记录的是sql执行相关的信息。当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作。

当从数据库读取数据时，会首先从Buffer Pool中读取，如果Buffer Pool中没有，则从磁盘读取后放入Buffer Pool；当向数据库写入数据时，会首先写入Buffer Pool，Buffer Pool中修改的数据会定期刷新到磁盘中。如果MySQL宕机，而此时Buffer Pool中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。

持久性实现：当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作；当**事务提交**时，会调用fsync接口对redo log进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。redo log采用的是WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。

为什么redo log比直接将Buffer Pool中修改的数据写入磁盘(即刷脏)要快呢？
（1）刷脏是随机IO，因为每次修改的数据位置随机，但写redo log是追加操作，属于顺序IO。
（2）刷脏是以数据页（Page）为单位的，MySQL默认页大小是16KB，一个Page上一个小修改都要整页写入；而redo log中只包含真正需要写入的部分，无效IO大大减少。

隔离性：
-   (一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性
-   (一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性

[MySQL当前读和快照读的例子](https://www.zhihu.com/question/47007926/answer/1523900737)

一致性：
一致性是指事务执行结束后，数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态。数据库的完整性约束包括但不限于：实体完整性（如**行的主键存在且唯一**）、列完整性（如**字段的类型、大小、长度要符合要求**）、外键约束、用户自定义完整性（如转账前后，两个账户余额的和应该不变）

### [binlog](https://zhuanlan.zhihu.com/p/33504555)🌟
[参考](https://www.zhihu.com/question/463438061)
binlog是Mysql sever层维护的一种二进制日志，与innodb引擎中的redo/undo log是完全不同的日志；其主要是用来记录对mysql数据更新或潜在发生更新的SQL语句，并以"事务"的形式保存在磁盘中；

作用：
Mysql binlog日志有ROW,Statement,MiXED三种格式；

**Row level:** 仅保存记录被修改细节，不记录sql语句上下文相关信息。【仅需记录哪条数据被修改了】
优点：不会发生某些特定情况下的procedure、function、及trigger的调用触发无法被正确复制的问题，任何情况都可以被复制，且能加快从库重放日志的效率，保证从库数据的一致性。
缺点：由于所有的执行的语句在日志中都将以每行记录的修改细节来记录，因此，可能会产生大量的日志内容，干扰内容也较多。

新版本的MySQL对row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到**表结构变更**的时候就会以statement模式来记录，如果sql语句确实就是update或者delete等修改数据的语句，那么还是会记录所有行的变更；

**Statement level:** 每一条**会修改数据的sql**都会记录在binlog中。
优点：只需要记录执行语句的细节和上下文环境，避免了记录每一行的变化，在一些修改记录较多的情况下相比ROW level能大大减少binlog日志量，节约IO，提高性能；还可以用于实时的还原；
缺点：为了保证sql语句能在slave上正确执行，必须记录上下文信息，以保证所有语句能在slave得到和在master端执行时候相同的结果；另外，主从复制时，存在部分函数（如sleep）及存储过程在slave上会出现与master结果不一致的情况。

**Mixedlevel level：**

### [redo log](https://juejin.cn/post/6987557227074846733)
update 操作其实是分为两步操作，先查询到对应的行记录，再根据条件进行更新操作。
如果没有 redo log 的话，MySQL 每次的update操作都要更新磁盘文件，更新磁盘文件需要先在磁盘中找到对应的行记录，再更新，每一条 update 语句都要操作磁盘文件，整个过程的 I/O 成本，查找成本都很高。
为了解决这个问题，InnoDB 引擎的设计者想到了一个办法，**先将记录写到 redo log 中，并更新内存，这个时候更新就算完成了**，操作内存比操作磁盘要快的多。同时，InnoDB 会在适当的时候，将 redo log 中的记录更新到磁盘文件中。这个更新往往是系统空闲时做。

每次更新操作都要往 redo log 中写入，如果 redo log 满了，空间不够用了怎么办？
InnoDB 的 redo log 文件是固定大小的，比如可以配置一组4个文件，每个文件大小是 1GB，那么 redo log 中可以记录 4GB 的操作，InnoDB 会从第一个文件开始写入，直到第四个文件写满了，又回到第一个文件开头循环写。

write pos和checkpoint之间的是 redo log上还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示 redo log 满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。

有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。

1. 数据更新到内存后先写redo log，处于prepare阶段
2. 写bin log
3. 提交事务，将 redo log 标记为 commit 状态

### redo log 和 bin log的区别
1. 作用不同：redo log是用于crash recovery的，保证MySQL宕机也不会影响持久性；binlog是用于point-in-time recovery的，保证服务器可以基于时间点恢复数据，此外binlog还用于主从复制。
2. 层次不同：redo log是InnoDB存储引擎实现的，而binlog是MySQL的服务器层(可以参考文章前面对MySQL逻辑架构的介绍)实现的，同时支持InnoDB和其他存储引擎。
3. 内容不同：redo log是物理日志，内容基于磁盘的Page；binlog的内容是二进制的，根据binlog_format参数的不同，可能基于sql语句、基于数据本身或者二者的混合。
4. 写入时机不同：binlog在事务提交时写入；redo log的写入时机相对多元：
	-   前面曾提到：当事务提交时会调用fsync对redo log进行刷盘；这是默认情况下的策略，修改innodb_flush_log_at_trx_commit参数可以改变该策略，但事务的持久性将无法保证。
	-   除了事务提交时，还有其他刷盘时机：如master thread**每秒刷盘一次**redo log等，这样的好处是不一定要等到commit时刷盘，commit速度大大加快。

### 并发事务带来的问题
丢失更新：最后的更新覆盖了其他事务之前的更新，而事务之间并不知道，发生更新丢失。更新丢失，可以完全避免，应用对访问的数据加锁即可。
[事务丢失更新的解决方法](https://blog.csdn.net/u014590757/article/details/79612858)

脏读：脏读指的是读到了其他事务未提交的数据，未提交意味着这些数据可能会回滚，也就是可能最终不会存到数据库中，也就是不存在的数据。读到了并不一定最终存在的数据，这就是脏读。

不可重复读：在同一事务内，不同的时刻读到的同一批数据可能是不一样的，可能会受到其他事务的影响，比如其他事务改了这批数据并提交了。通常针对数据更新（UPDATE）操作。
【脏读与不可重复读的区别在于：前者读到的是其他事务未提交的数据，后者读到的是其他事务已提交的数据。】

不可重复读表达的是记录（一行或多行）的值在同一次事务中出现两个不同的结果。幻读表达的是：同一事务中查询两次得到两个不同的结果集（）。

幻读：幻读是针对数据插入（INSERT）操作来说的。假设事务A对某些行的内容作了更改，但是还未提交，此时事务B插入了与事务A更改前的记录相同的记录行，并且在事务A提交之前先提交了，而这时，在事务A中查询，会发现好像刚刚的更改对于某些数据未起作用，但其实是事务B刚插入进来的。

![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/gljb.png)

### 幻读
[参考](https://opensource.actionsky.com/20210818-mysql/)
事务 T1 读取一组满足某些 <搜索条件> 的数据。事务 T2 创建了满足 T1 的 <搜索条件> 的数据项并提交。如果 T1 用相同的<搜索条件>再次读取，得到一组不同于第一次读取的数据。这就叫幻读。

幻读定义中 T2 是“创建数据”，不可重复读的定义中 T2 是修改或者删除数据。

在满足 < search condition > 的范围内，**修改和删除数据必定是对已经存在的数据行操作，而创建数据则意味着创建之前这个数据项是不存在的。“创建数据”不仅是 insert，还包括 update。update 把本来不满足谓词范围的数据项更新成满足谓词范围的数据项。**

- read-uncommitted 可能会出现脏读；
- read-committed 可能会出现不可重复读；
- repeatable-read 论文里可能会出现幻读；【但是Mysql通过间隙锁和MVCC分别解决了当前读和快照读下的幻读问题】

SERIALIZABLE 隔离级别，是通过锁来实现的，参考上面基于锁定义的隔离级别：所有读都会对谓词范围加长锁，直到事务终止；
其他隔离级别，是用 MVCC 实现的。普通的 select 称为快照读，是不会加读锁的，目的是为了提升读写并发性能。

- MVCC 能实现**快照读**的“可重复读”，不会出现“不可重复读”和“幻读”
- 间隙锁（或者说 next-key lock）实现了**当前读**的“可重复读”，也不会出现“不可重复读”和“幻读”
-  快照读和当前读混用造成的异常，不能算是幻读【当前读的效果就是要读取最新版本，实际上是把隔离级别从 repeatable-read 降级到了 read-committed，所以快照读和当前读混用不算幻读。】

### 事务隔离级别🌟🌟🌟🌟🌟
**事务隔离分为不同级别**，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。

**Read Uncommitted**
一个事务还没提交时，它做的变更就能被别的事务看到，读取尚未提交的数据，哪个问题都不能解决；

**Read Committed**
一个事务提交之后，它做的变更才会被其他事务看到，读取已经提交的数据，可以解决脏读。

**Repeatable Read**
一个事务A在事务执行过程中第一次读取的值和第二次读取的值一致（解决了不可重复读），但是其他事务B 的insert 或者 delete的操作，会影响到俩次查询的条数（现象：幻读）

一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的，可以解决脏读和不可重复读。

**Serializable**
对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

### 隔离级别的实现原理
未提交读：
事务对当前被读取的数据不加锁；  
事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加**行级共享锁**，直到事务结束才释放。

提交读：
事务对当前被读取的数据加**行级共享锁（当读到时才加锁）**，一旦读完该行，**立即释放**该行级共享锁；
事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加**行级排他锁**，直到事务结束才释放。

可重复读：
事务在读取某数据的瞬间（就是开始读取的瞬间），必须先对其加**行级共享锁**，**直到事务结束才释放**；  
事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加**行级排他锁**，直到事务结束才释放。

序列化读：
事务在读取数据时，必须先对其加表级共享锁 ，直到事务结束才释放；
事务在更新数据时，必须先对其加表级排他锁 ，直到事务结束才释放。

为了不加锁解决读写冲突的问题，MySQL引入了MVCC机制。

[参考](https://blog.51cto.com/u_15265854/2891760)
>如果使用锁机制来实现这两种隔离级别，在可重复读中，该sql第一次读取到数据后，就将这些数据加锁，其它事务无法修改这些数据，就可以实现可重复读了。但这种方法却无法锁住insert的数据，所以当事务A先前读取了数据，或者修改了全部数据，事务B还是可以insert数据提交，这时事务A就会发现莫名其妙多了一条之前没有的数据，这就是幻读，不能通过行锁来避免。需要Serializable隔离级别 ，读用读锁，写用写锁，读锁和写锁互斥，这么做可以有效的避免幻读、不可重复读、脏读等问题，但会极大的降低数据库的并发能力。

### 事务实现原理🌟🌟
[参考](https://segmentfault.com/a/1190000039139809)

原子性： undo log

持久性：redo log

隔离性：
(一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性
(一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性
通过锁机制、数据的隐藏列、undo log和类next-key lock，实现了一定程度的隔离性

一致性：
**数据库本身提供保障，例如不允许向整型列插入字符串值、字符串长度不能超过列的限制等**
应用层面进行保障，例如如果转账操作只扣除转账者的余额，而没有增加接收者的余额，无论数据库实现的多么完美，也无法保证状态的一致

### MVCC三个隐藏列
1. DB_TRX_ID 事务id：占6 字节，表示这一行数据**最后插入或修改**的事务id。此外删除在内部也被当作一次更新，在行的特殊位置添加一个删除标记（记录头信息有一个字节存储是否删除的标记）。
2. DB_ROLL_PTR 回滚指针：占7字节，回滚指针指向被写在Rollback segment中的undoLog记录，在该行数据被更新的时候，undoLog 会记录该行修改前内容到undoLog。
3. DB_ROW_ID 行ID：占7字节，他就像自增主键一样随着插入新数据自增。如果表中不存**主键**或者**唯一索引**，那么数据库就会采用DB_ROW_ID生成聚簇索引。否则DB_ROW_ID不会出现在索引中。

### MVCC(Multi-Version Concurrency Control)🐨🐋🌟
在同一时刻，不同的事务读取到的数据可能是不同的(即多版本)——在T5时刻，事务A和事务C可以读取到不同版本的数据。

![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/MVCC.png)

MVCC最大的优点是**读不加锁，因此读写不冲突**，并发性能好。InnoDB实现MVCC，多个版本的数据可以共存，主要基于以下技术及数据结构：
1. 隐藏列：InnoDB中每行数据都有隐藏列，隐藏列中包含了本行数据的事务id、指向undo log的指针等。
2. 基于undo log的版本链：前面说到每行数据的隐藏列中包含了指向undo log的指针，而每条undo log也会指向更早版本的undo log，从而形成一条版本链。
3. ReadView：通过隐藏列和版本链，MySQL可以将数据恢复到指定版本；但是具体要恢复到哪个版本，则需要根据ReadView来确定。所谓ReadView，是指事务（记做事务A）在某一时刻给整个事务系统（trx_sys）打快照，之后再进行读操作时，会将读取到的数据中的事务id与trx_sys快照比较，从而判断数据对该ReadView是否可见，即对事务A是否可见。

可见性判定：
-   low_limit_id：表示生成ReadView时系统中应该**分配给下一个事务的id**。如果数据的事务id大于等于low_limit_id，则对该ReadView不可见。
-   up_limit_id：表示生成ReadView时当前系统中**活跃的读写事务中最小的事务id**。如果数据的事务id小于up_limit_id，则对该ReadView可见。
-   rw_trx_ids：表示生成ReadView时当前系统中**活跃的读写事务的事务id列表**。如果数据的事务id在low_limit_id和up_limit_id之间，则需要判断事务id是否在rw_trx_ids中：如果在，说明生成ReadView时事务仍在活跃中，因此数据对ReadView不可见；如果不在，说明生成ReadView时事务已经提交了，因此数据对ReadView可见。

1.  当前事务内的更新，可以读到；
2.  版本未提交，不能读到；
3.  版本已提交，但是却在快照创建后提交的，不能读到；
4.  版本已提交，且是在快照创建前提交的，可以读到；

前面介绍的MVCC，是RR隔离级别下“非加锁读”实现隔离性的方式。

```sal
select @@global.tx_isolation; # 全局隔离级别
select @@tx_isolation; # 本次会话的隔离级别
show variables like 'autocommit';
```

RC与RR一样，都使用了MVCC，其主要区别在于：
1. RR是**在事务开始后第一次执行select前创建ReadView**，直到事务提交都不会再创建。根据前面的介绍，RR可以避免脏读、不可重复读和幻读。
2. RC**每次执行select**前都会重新建立一个新的ReadView，因此如果事务A第一次select之后，事务B对数据进行了修改并提交，那么事务A第二次select时会重新建立新的ReadView，因此事务B的修改对事务A是可见的。因此RC隔离级别可以避免脏读，但是无法避免不可重复读和幻读。

按照是否加锁，MySQL的读可以分为两种：
1. 非加锁读，也称作快照读、一致性读，使用普通的select语句，这种情况下使用MVCC避免了脏读、不可重复读、幻读，保证了隔离性。
2. 加锁读。

```sql
select ... lock in share mode; # 共享锁读取
select ... for update; # 排他锁读取
```

实际上insert undo只在事务回滚时起作用，当事务提交后，该类型的undo日志就没用了，它占用的Undo Log Segment也会被系统回收。

![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/kzyz.png)

InnDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正**活跃（还未提交）的事务**。事务 ID 随时间严格递增的，**把系统中已提交的事务 ID 的最大值记为数组的低水位，已创建过的事务 ID + 1记为高水位**。

1 如果 trx_id 在灰色区域，表明被访问版本的 trx_id 小于数组中低水位的 id 值，也即生成该版本的事务在生成 read view 前已经提交，所以该版本可见，可以被当前事务访问。
2 如果 trx_id 在橙色区域，表明被访问版本的 trx_id 大于数组中高水位的 id 值，也即生成该版本的事务在生成 read view 后才生成，所以该版本不可见，不能被当前事务访问。
3 如果在绿色区域，就会有两种情况：【可以理解为事务是按照时间顺序排序的，在绿色区域中有未提交的事务排在已提交事务的前面，在数组中说明这个事务未提交】
	a) trx_id 在数组中，证明这个版本是由还未提交的事务生成的，不可见
	b) trx_id 不在数组中，证明这个版本是由已提交的事务生成的，可见

### ReadView存放在哪里？
InnoDB的MVCC是通过在每行记录后面保存两个隐藏的列来实现的。一个保存了行的事务ID（DB_TRX_ID），一个保存了行的回滚指针（DB_ROLL_PT）。undo log保存的是一个版本链，也就是使用DB_ROLL_PTR这个字段来连接的。

对于 RC(READ COMMITTED) 和 RR(REPEATABLE READ) 隔离级别的实现就是通过上面的版本控制来完成。两种隔离界别下的核心处理逻辑就是判断所有版本中哪个版本是当前事务可见的处理。针对这个问题InnoDB在设计上增加了ReadView的设计，ReadView中主要包含当前系统中还有哪些活跃的读写事务，把它们的事务id放到一个列表中，我们把这个列表命名为为m_ids。

### 不可重复读是什么
不可重复读，是指在数据库访问中，一个事务范围内两个相同的查询却返回了不同数据。这是由于查询时系统中其他事务修改的提交而引起的。比如事务T1读取某一数据，事务T2读取并修改了该数据，T1为了对读取值进行检验而再次读取该数据，便得到了不同的结果。

### 幻读举例
例如事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作，这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有一行没有修改，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。

幻读出现的场景：事务的隔离级别为可重复读。

不可重复读重点在于update和delete，而幻读的重点在于insert。
当事务A先前读取了数据，或者修改了全部数据，事务B还是可以insert数据提交，这时事务A就会发现莫名其妙多了一条之前没有的数据，这就是幻读。

【删除应该不算幻读，幻读应该特指加入行。delete和update可以对记录加锁，保证事务安全。而insert，由于插入行不存在，无法加锁，只能引入间隙锁解决，这也是幻读单独拿出来的原因。】

### MySQL如何保证可重复读?
可重复读是指：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。
MVCC

### 删除会出现幻读吗？
DELETE：将当前事务的版本号保存至行的删除版本号。

### innodb如何解决幻读
由于锁的特性，当某事务对数据进行加锁读后，其他事务无法对数据进行写操作，因此可以避免脏读和不可重复读。而避免幻读，则需要通过next-key lock。next-key lock是行锁的一种，实现相当于record lock(记录锁) + gap lock(间隙锁)；其特点是不仅会锁住记录本身(record lock的功能)，还会锁定一个范围(gap lock的功能)。因此，加锁读同样可以避免脏读、不可重复读和幻读，保证隔离性。

概括来说，InnoDB实现的RR，通过锁机制（包含next-key lock）、MVCC（包括数据的隐藏列、基于undo log的版本链、ReadView）等，实现了一定程度的隔离性，可以满足大多数场景的需要。不过需要说明的是，RR虽然避免了幻读问题，但是毕竟不是Serializable，不能保证完全的隔离。

如果在事务中第一次读取采用非加锁读，第二次读取采用加锁读，则如果在两次读取之间数据发生了变化，两次读取到的结果不一样，因为加锁读时不会采用MVCC。

MySQL为了减少锁处理（包括等待其它锁）的时间，提升并发能力，引入了**快照读**的概念，**使得select不用加锁**。而update、insert这些“当前读”，就需要另外的模块来解决了。为了解决当前读中的幻读问题，MySQL事务使用了Next-Key锁。

行锁防止别的事务修改或删除，GAP锁防止别的事务新增，行锁和GAP锁结合形成的的Next-Key锁共同解决了RR级别在**写数据**时的幻读问题。【有索引的情况下，给记录两边的GAP加锁。如果 不是索引列，那么数据库会为整个表加上间隙锁。】

![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/gap.png)

在Serializable这个级别，select还是会加锁的！

### 间隙锁
间隙锁是封锁索引记录中的间隔，或者第一条索引记录之前的范围，又或者最后一条索引记录之后的范围。

间隙锁（Gap Lock）是Innodb在可重复读提交下为了解决幻读问题时引入的锁机制。

**间隙锁能完全避免幻读么？**

[参考](https://www.jianshu.com/p/32904ee07e56)

### LBCC
LBCC解决的是当前读情况下的幻读，MVCC解决的是普通读（快照读）的幻读。
LBCC是Lock-Based Concurrent Control的简称，意思是基于锁的并发控制。

上图中的（-∞，1）、（1，5）...（11，+∞）为数据库中存在的间隙。而（-∞，1]、（1，5]...（11，+∞）我们称之为临键(next-key lock)，即左开右闭的集合。

### MVCC和间隙锁的区别
[参考](https://blog.csdn.net/weixin_43705457/article/details/104849943)
在快照读时使用MVCC,在当前读时使用next_key锁
- 快照读：读取的是记录的可见版本（有可能是历史版本），不加锁。
	场景：select
- 当前读：读取的是记录的最新版本，并且当前读返回的记录会加锁，保证其他事务不会再并发修改这条记录。
	场景：update、insert、delete

MVCC最大的好处是读不加锁，读写不冲突

### MySQL读写锁
[MySQL读写锁](https://learnku.com/articles/39212?order_by=vote_count&)

### MySQL怎么实现主从复制?
[MySQL怎么实现主从复制?](https://blog.nowcoder.net/n/b90c959437734a8583fddeaa6d102e43#%E4%BA%8C%E3%80%81MySQL%20%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E7%9A%84%E6%A6%82%E5%BF%B5)

当从节点连接主节点时，主节点会为其创建一个log dump 线程，用于发送和读取binlog的内容。在读取binlog中的操作时，log dump线程会对主节点上的binlog加锁，当读取完成，在发送给从节点之前，锁会被释放。主节点会为自己的每一个从节点创建一个log dump 线程。

当从节点上执行`start slave`命令之后，从节点会创建一个I/O线程用来连接主节点，请求主库中更新的binlog。I/O线程接收到主节点的log dump进程发来的更新之后，保存在本地relay-log（中继日志）中。

SQL线程负责读取relay-log中的内容，解析成具体的操作并执行，最终保证主从数据的一致性。

MySQL 主从复制有三种方式：基于SQL语句的复制（statement-based replication，SBR），基于行的复制（row-based replication，RBR)，混合模式复制（mixed-based replication,MBR)。对应的binlog文件的格式也有三种：STATEMENT,  ROW,  MIXED。

MySQL master将SQL语句分解为基于Row更改的语句并记录在binlog中，也就是只记录哪条数据被修改了，修改成什么样。

**简单的同步过程：**

Slave 连接到 Master;
Master 开启 binlog dump 线程;
Dump 线程将 binlog 内容推给 Slave 的 IO 线程;
Slave 将内容写入到 relay log;
Slave SQL 线程读取 relay log,解析并重放执行,实现数据库同步;

通常master节点复制数据到slave节点，有几种方式：
1. 同步复制：当主库执行完一个事务，**所有的从库都执行了该事务**才返回给客户端。因为需要等待所有从库执行完该事务才能返回。
2. 异步复制：主库在执行完客户端提交的事务后会立即将结果返回给客户端，并不关心从库是否已经接收并处理。
3. 半同步复制：主库在执行完客户端提交的事务后不是立刻返回给客户端，而是等待**至少一个从库接收到并写到relay log**中才返回给客户端。

**另一种答案**
1. Master将数据改变记录到二进制日志(binary log)中
2. Slave上面的IO进程连接上Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容
3. Master接收到来自Slave的IO进程的请求后，负责复制的IO进程会根据请求信息读取日志指定位置之后的日志信息，返回给Slave的IO进程。
4. Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并**将读取到的Master端的 bin-log的文件名和位置记录到master-info文件中**，以便在下一次读取的时候能够清楚的告诉Master从某个bin-log的哪个位置开始往后的日志内容
5. Slave的Sql进程检测到relay-log中新增加了内容后，会马上解析relay-log的内容成为在Master端真实执行时候的那些可执行的内容，并在自身执行

### MySQL主从同步的三种模式
种模式：异步复制、半同步复制、全同步复制

异步复制是mysql 默认的同步方式。
slave会启动两个线程，IO Thread 和 SQL Thread，IO Thread 负责从master拉取binlog 日志，并写入relay中继日志；SQL Thread 负责将relay中继日志中的变更进行重放，更新数据来达到跟master保持数据一致的目的；这个过程中，slave通过IO线程拉取binlog，master无需关注是否有slave需要同步，只做自己的事情，整个复制过程都是异步完成的，这个就是异步复制。

半同步复制
![](image/半同步复制.png)

master更新操作写入binlog之后会主动通知slave，slave接收到之后写入relay log 即可应答，master只要收到至少一个ack应答，则会提交事务。相比较于异步复制，半同步复制需要依赖**至少一个slave将binlog写入relay log**，在性能上有所降低，但是可以保证至少有一个从库跟master的数据是一致的，数据的安全性提高。

全同步复制：
全同步复制必须收到所有从库的ack，才会提交事务。

### 分布式系统的CAP理论
一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。
一致性指“all nodes see the same data at the same time”，即所有节点在同一时间的数据完全一致。
可用性指“Reads and writes always succeed”，即服务在正常响应时间内一直可用。
分区容错性指分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务。

假设在N1和N2之间网络断开的时候，有用户向N1发送数据更新请求，那N1中的数据V0将被更新为V1，由于网络是断开的，所以分布式系统同步操作M，所以N2中的数据依旧是V0；这个时候，有用户向N2发送数据读取请求：
有二种选择，第一，牺牲数据一致性，响应旧的数据V0给用户；第二，牺牲可用性，阻塞等待，直到网络连接恢复，数据更新操作M完成之后，再给用户响应最新的数据V1。

BASE理论是一致性和可用性的权衡结果。**BASE** 是 **Basically Available（基本可用）** 、**Soft-state（软状态）** 和 **Eventually Consistent（最终一致性）** 三个短语的缩写。
即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。

### CAP中的C和ACID中的C有区别吗?
ACID 中的 C 是关系数据库中为了保证事务所必须满足的一个条件，指在事务开始之前和结束之后，数据库的完整性没有被破坏，写入的数据必须完全符合所有的预设约束、触发器、联机回滚等。
CAP 中的 C 是分布式系统中不能同时满足的三个条件之一，指所有节点都存储最新的数据备份。

### 事务回滚的实现
我们这里，之所以能够保证原子性，则是靠undo log。当事务对数据库进行修改时，InnDB会生成对应的undo log；如果事务失败或者调用了rollback，导致事务回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。

undo log属于**逻辑日志**，它记录的是sql执行相关的信息。当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。

### [双主架构遇到的问题](https://www.cnblogs.com/exceptioneye/p/5060967.html)
双主复制需要注意哪些问题？数据完整性、数据一致性和主键冲突

1. 尽可能通过业务场景设计来规避，对于同一个库，同一张表，同一个记录中的同一字段的两地变更，引发的数据一致性判断冲突。
2. **避免使用数据库自增类主键方案，而使用分布式全局ID，避免双主双写并同步复制可能引发主键冲突。**
3. 双向同步复制Dual Master潜在可能引发循环同步的问题。

### [MySQL双主架构在不分表的情况下如何保证数据一致性？](https://blog.csdn.net/followmyinclinations/article/details/52870418)
MySQL最常见的集群架构，是**一主多从，主从同步，读写分离**的架构。通过这种方式，能够扩充数据库的读性能，保证读库的高可用，但此时写库仍然是单点。

为了保证MySQL写库的高可用，可以在一个MySQL数据库集群中可以设置两个主库，并设置双向同步，以冗余写库的方式，来保证写库的高可用。

**出现不一致的原因：**
假如Master A和Master B几乎同时对一条数据进行了更新，对Master A的更新比对Master B的稍微快一点点，当对Master Ａ的更新最终同步到Master B时， 老版本的数据会把版本更新的数据覆盖，而且不会抛出异常，从而导致数据不一致的现象发生。

采用的是[**Vector Clocks**](http://edisonxu.com/2018/11/02/clocks.html)来解决一致性的。
每个机器维护一个向量VC，也就是Vector Clock，这个向量VC有如下属性：
-   VCi[i] 是到目前为止机器i上发生的事件的个数
-   VCi[k] 是机器i知道的机器k发生的事件的个数(即机器i对机器j的知识)

通过以下步骤更新：
1. 机器i本地发生一个事件时将VCi[i]加1  
2. 机器i给机器j发送消息m时，将整个VCi存在消息内  
3. 机器j收到消息m时，VCj[k]=max(VCj[k],VCi[k]),同时，VCj[j]+1

如果Object的版本A的VCA包含的每项(server, counter)在版本B的VCB中都有对应项，并且counter小于等于版本B中对应项的counter(记作VCB descends VCA)，那么这个Object的版本A可以被丢弃，否则需要对两个版本进行merge。

如果有冲突，不是descends的关系，那么就两个版本都保留，当客户端来读Key K的时候，两个版本的数据和对应的VC都返回给客户端，由客户端进行冲突合并，客户端进行冲突合并后写入Key K的时候，带着合并后的VC[(M1, 1), [M2, 1]]发到M1/M2，覆盖服务器版本，冲突解决。

### 如何避免MySQL双主架构出现会循环的数据更新
**Dual Master：**
两个MySQL Server互相将对方作为自己的Master，自己作为对方的Slave来进行复制。这样，任何一方所做的变更，都会通过复制应用到另外一方的数据库中。

在MySQL的Binary Log中记录了当前MySQL的server-id，而且这个参数也是搭建MySQL Replication的时候必须明确指定的，只有Master和Slave的server-id参数值不一致时MySQL Replication才能搭建成功。一旦有了server-id的值，MySQL就很容易判断某个变更是从哪一个MySQL Server最初产生的，所以就很容易避免出现循环复制的情况。

### 外键约束的作用
通过定义外键约束，关系数据库可以保证无法插入无效的数据。
由于外键约束会降低数据库的性能，大部分互联网应用程序为了追求速度，并不设置外键约束，而是仅靠应用程序自身来保证逻辑的正确性。

### 在InnoDB中一个3层B+树最多大概可以存放多少行数数据？
在innodb存储引擎里面，最小的存储单元是页（page），一个页的大小是16KB。

```mysql
show variables like 'innodb_page_size';
```

数据表中的数据都是存储在页中的，所以一个页中能存储多少行数据呢？
[参考](https://blog.csdn.net/VitaminX181/article/details/117913570)
1000* 1000* 500

### MySQL单表能存储多少条数据？
[参考](https://juejin.cn/post/6930807753396977677)
MySQL的数据页大小是16KB。（确切的说是InnoDB数据页大小16KB）。
千万级别

### MySQL索引🌟🌟
ALTER TABLE tbl_name ADD INDEX index_name (column_list): 添加普通索引
[参考](https://developer.huawei.com/consumer/cn/forum/topic/0204405591412170236)
按数据结构分类可分为：B+tree索引、Hash索引、Full-text索引。
按物理存储分类可分为：聚簇索引、二级索引（辅助索引）。
按字段特性分类可分为：主键索引、普通索引、前缀索引。
按字段个数分类可分为：单列索引、联合索引（复合索引、组合索引）。
***
[MySQL索引分类，90%的开发都不知道](http://blog.itpub.net/69940575/viewspace-2682187/)
根据底层划分：
- Hash索引：在MySQL的Innodb里，对于**热点的数据**会自动生成Hash索引。
- B+树索引

根据索引字段划分：
- 单值索引
- 联合索引(复合索引)：复合索引的索引的数据顺序跟字段的顺序相关，包含多个值的索引中，如果当前面字段的值重复时，将会按照其后面的值进行排序。

根据是否是在主键上建立的索引进行划分：
- 主键索引：MySQL中是根据主键来组织数据的，所以每张表都必须有主键索引，主键索引只能有一个，不能为null同时必须保证唯一性。**建表时如果没有指定主键索引，则会自动生成一个隐藏的字段作为主键索引。**
- 辅助索引(二级索引)：主键索引的叶子节点存储了完整的数据行，而非主键索引的叶子节点存储的则是主键索引值，通过非主键索引查询数据时，会先查找到主键索引，然后再到主键索引上去查找对应的数据。
- 覆盖索引： 索引信息里完全包含我们所要的信息，如果能从辅助索引里返回name信息，则第二步（回查）是完全没有必要的，可以极大提升查询速度。Innodb里针对使用辅助索引的查询场景做了优化，叫覆盖索引。

根据数据与索引的存储关联性划分：
- 聚簇索引：Innodb的主键索引，非叶子节点存储的是索引指针，叶子节点存储的是既有索引也有数据，是典型的聚簇索引。
- 非聚簇索引：MyISAM中索引和数据文件分开存储，B+Tree的叶子节点存储的是数据存放的地址，而不是具体的数据，是典型的非聚簇索引；换言之，数据可以在磁盘上随便找地方存，索引也可以在磁盘上随便找地方存，只要叶子节点记录对了数据存放地址就行。另外Inndob里的辅助索引也是非聚簇索引。

其他分类：
- 唯一索引：主键索引一定是唯一索引，而唯一索引不一定是主键索引。唯一索引可以理解为仅仅是将索引设置一个唯一性的属性。
- 全文索引：在定义的值中支持全文查找，允许空值和重复值，可以在CHAR，VARCHAR或者TEXT字段类型上创建，仅支持MyISAM存储引擎。
- 空间索引：针对空间数据做的索引，支持的数据类型有4种，分别是GEOMETRY，POINT，LINESTRING和POLYGON。创建空间索引的列必须声明为非空值（NOT NULL），仅支持MyISAM存储引擎。

### 主键索引与唯一索引的区别
- 主键创建后一定包含一个唯一性索引，唯一性索引并不一定就是主键。
- **唯一性索引列允许空值，而主键列不允许为空值。**
- 主键可以被其他表引用为外键，而唯一索引不能。
- 一个表最多只能创建一个主键，但可以创建多个唯一索引。
- 主键更适合那些不容易更改的唯一标识，如自动递增列。

### MySQL索引的优缺点
索引的优点：
1. 通过创建唯一索引，可以保证数据库每一行数据的唯一性
2. 可以大大提高查询速度
3. 可以加速表与表的连接
4. 可以显著的减少查询中分组和排序的时间。

索引的缺点:
1. 创建索引和维护索引需要时间，而且数据量越大时间越长。
2. 创建索引需要占据磁盘的空间，如果有大量的索引，可能比数据文件更快达到最大文件尺寸。
3. 当对表中的数据进行增加，修改，删除的时候，索引也要同时进行维护，降低了数据的维护速度。

### 索引失效的几种情况
[参考](https://blog.51cto.com/u_14479502/3116862)
- 如果条件中有or，即使其中有部分条件带索引也不会使用(这也是为什么尽量少用or的原因)，如果要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引；或者使用union all
- 对于复合索引，如果不使用前列，后续列也将无法使用索引，复合索引需要满足**最左匹配**原则；
- 模糊查询like以%开头；
- 索引列的数据存在数据类**型隐形转换**，则用不上索引，比如列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引；
- where子句里对索引列上有**数学【表达式】运算**，用不上索引；
- where 子句里对有索引列使用**函数**，用不上索引；
- 如果mysql估计使用**全表扫描**要比使用索引快,则不使用索引。

什么情况下不推荐使用索引：
- 数据区分度较低（一个字段的取值只有几种时，例如性别字段）的字段不要使用索引；
- 频繁更新的字段不要使用索引；
- 字段不在where语句出现时不要添加索引，如果where后含IS NULL 、IS NOT NULL、 like ‘%输入符%’等条件，不建议使用索引；
- where 子句里对索引列使用不等于（<>），使用索引效果一般。

### 索引优化🐨🌟
- 如果MySQL估计使用索引比全表扫描还慢，则不会使用索引。
- 前导模糊查询不能命中索引（%like%）。非前导模糊查询则可以使用索引。
- 数据类型出现隐式转换的时候不会命中索引，特别是当列类型是字符串，一定要将字符常量值用引号引起来。
- 复合索引的情况下，查询条件不包含索引列最左边部分（不满足最左原则），不会命中复合索引。最左原则并不是说是查询条件的顺序，而是**查询条件中是否包含索引最左列字段**。
- union、in、or都能够命中索引，建议使用in。
- 用or分割开的条件，如果or前的条件中列有索引，而后面的列中没有索引，那么涉及到的索引都不会被用到。
- **负向条件查询**不能使用索引，可以优化为in查询。负向条件有：!=、<>、not in、not exists、not like等。
- 范围条件查询可以命中索引。范围条件有：<、<=、>、>=、between等。
- 数据库执行计算不会命中索引。
- 利用覆盖索引进行查询，避免回表。
- 建立索引的列，不允许为null。

### 非聚簇索引的查询过程
第一步在辅助索引B+树中检索Name，到达其叶子节点获取对应的主键。第二步使用主键在主索引B+树种再执行一次B+树检索操作，最终到达叶子节点即可获取整行数据。

### 创建索引的原则/如何建立索引🐋
[创建索引的原则](https://zhuanlan.zhihu.com/p/88963084)
1. **最左前缀匹配原则**，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。
2. =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。
3. 尽量选择**区分度高**的列作为索引，区分度的公式是count(distinct col)/count(\*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录。
4. 索引列**不能参与计算**，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’)。
5. 尽量的**扩展索引**，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。

索引的设计原则：
1. 不是越多越好
2. 常更新的表越少越好
3. 数据量小的表最好不要建立索引
4. 不同的值比较多的列才需要建立索引
5. 某种数据本身具备**唯一性**的时候，建立唯一性索引，可以保证定义的列的数据完整性，以提高查询速度
6. **频繁进行排序或分组的列**(group by或者是order by)可以建立索引，提高搜索速度
7. **经常用于查询条件**的字段应该建立索引

### SQL优化🐋🐋
[参考](https://zhuanlan.zhihu.com/p/483334366)
- 索引列不能参与计算，禁止对索引字段使用函数、运算符操作，会使索引失效
- 联合索引注意最左匹配原则：必须按照从左到右的顺序匹配，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配
- 在**区分度高**的字段上面建立索引可以有效的使用索引，区分度太低，无法有效的利用索引，可能需要扫描所有数据页，此时和不使用索引差不多
- 查询记录的时候，少使用*，尽量去利用**索引覆盖**，可以减少回表操作，提升效率
- 有些查询可以采用联合索引，进而使用到索引下推（IPC），也可以减少回表操作，提升效率
- 字符串字段和数字比较的时候会使索引无效
- 模糊查询’%值%'会使索引无效，变为全表扫描，但是’值%'这种可以有效利用索引
- 排序中尽量使用到索引字段，这样可以减少排序，提升查询效率
- 使用与业务无关的自增主键作为主键，如果主键频繁改变(update)，物理顺序会改变，MySQL要不断调整B+树，并且中间可能会产生页面的分裂和合并等等，会导致性能会急剧降低。
- join 语法，尽量将小的表放在前面，在需要on的字段上，数据类型保持一致，并设置对应的索引，否则MySQL无法使用索引来join查询
- 尽量的扩展索引，不要新建索引。
- 尽量避免使用！= 或 <>操作符，否则数据库引擎会放弃使用索引而进行全表扫描。使用>或<会比较高效。
- 尽量避免在where子句中对字段进行null值判断，否则将导致引擎放弃使用索引而进行全表扫描。
***
[参考](https://www.cnblogs.com/SimpleWu/p/9929043.html)
[为什么不用NULL](https://cloud.tencent.com/developer/article/1044026)：
MySQL难以优化引用可空列查询，它会使索引、索引统计和值更加复杂。可空列需要更多的存储空间，还需要mysql内部进行特殊处理。可空列被索引后，每条记录都需要一个额外的字节，还能导致MyISAM 中固定大小的索引变成可变大小的索引。【NULL 列在行中需要额外的空间来记录它们的值是否为 NULL。】

[SQL性能优化的策略有哪些？](https://www.zhihu.com/question/445155934/answer/2434274646)
1. 在使用联合索引的时候要特别注意最左前缀原则
2. 不在索引列上做任何操作(计算、函数、(自动/手动)类型转换)，会导致索引失效而转向全表扫描
3. 存储引擎不能使用索引中范围条件右边的列
4. 尽量使用覆盖索引，减少select \*语句
5. MySQL 在使用不等于( != 或者 <> )，not in，not exists 的时候无法使用索引会导致全表扫描
6. is null 和 is not null 一般情况下也无法使用索引，不回表或者需要回表的次数非常少，还是可以走索引的
7. like 以通配符开头(’%itwxe…’) MySQL 索引失效会变成全表扫描操作
8. 类型不匹配 MySQL 自动转型导致索引失效【字符串不加单引号索引失效，亦或是数值类型加单引号索引失效】
9. 少用 or 或 in ，用它查询时，MySQL 不一定使用索引
10. 如果子查询得出的结果集记录较少，主查询中的表较大且又有索引时应该用 in， 反之如果外层的主查询记录较少，子查询中的表大，又有索引时使用 exists。[参考](https://www.cnblogs.com/hider/p/12446035.html#%E4%B8%89in-%E4%B8%8E-exists-%E7%9A%84%E5%8C%BA%E5%88%AB)

### SQL优化
[参考](https://developer.huawei.com/consumer/cn/forum/topic/0201544730140730212)
- 查询SQL尽量不要使用select * ，而是具体字段
- 避免在where子句中使用or来连接条件，而是使用union all
- 使用varchar代替char
- 尽量使用数值替代字符串类型
- 查询尽量避免返回大量数据，通常采用分页
- 优化like语句，%不放在第一位
- 避免字符串和数字的隐式转换
- 索引不宜太多，索引太多虽然提高了查询的效率，但却会降低插入和更新的效率
- 索引不适合建在有大量重复数据的字段上
- 避免在索引列上使用内置函数
- 避免在where中对字段进行表达式操作
- 避免在where子句中使用!=或<>操作符
- where中使用默认值代替null
- 没有出现左边的字段，则不满足最左特性，索引失效
- 为where和order by中常出现的字段就创建索引
- 删除冗余和重复的索引

### 索引失效的情况
[参考](https://blog.csdn.net/weixin_40335368/article/details/116593321)
- 条件中有or
- like以%开头
- 存在索引列的数据类型隐式转换
- where子句中对索引列有数学运算
- where 子句里对有索引列使用函数
- 复合索引未用左列字段
- 辅助索引查询的语句过多？

### 多表查询优化
Nested Loop Join 实际上就是通过驱动表的结果集作为循环基础数据，然后一条一条的通过该结果集中的数据作为过滤条件到下一个表中查询数据，然后合并结果。
- 使用join代替子查询
- 应尽量避免在 where 子句中对字段进行 null 值判断，否则当命中数量大于特定值时会不走索引【只有不回表或者需要回表的次数非常少才走索引】[参考](https://blog.csdn.net/imaginehero/article/details/119418950)
- 尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。
- 对于 exists 来说，外表总会执行全表扫描的，外层大表内层小表，用in。外层小表内层大表，in和exists效率差不多。
- 用小表做驱动表，一定用上被驱动表的索引。

### MySQL联合索引查询时需要注意哪些问题

### 慢SQL排查和优化🐋
[慢SQL优化](https://juejin.cn/post/7048974570228809741)
1. 通过**慢查询日志**去寻找哪些 SQL 执行效率低
	- slow_query_log：是否开启慢日志查询，1表示开启，0表示关闭
	- slow_query_log_file：MySQL数据库慢查询日志存储路径
	- long_query_time：慢查询阈值，当SQL语句查询时间大于阈值，会被记录在日志上
1. 使用 explain 获取低效率 SQL 的**执行计划**
2. 结合 SQL 与执行计划，进行分析与优化

引起 SQL 查询很慢的原因与解决办法：
1. 没有索引。解决办法：
	- 根据 where 和 order by 使用比较频繁的字段创建索引，提高查询效率
	- 索引不宜过多，单表最好不要超过 6 个。索引过多会导致占用存储空间变大；insert、update 变慢
	- 删除未使用的索引
2. 索引未生效。解决办法：
	- 避免在 where 子句中对字段进行 null 值判断，创建表默认值是 NULL。尽量使用 NOT NULL，或使用特殊值，如 0、-1
	- 避免在 where 子句中使用 != 或 <> 操作符， MySQL 只有对以下操作符才使用索引：<、<=、=、>、>=、BETWEEN、IN、非 % 开头的 LIKE
	- 避免在 where 子句中使用 or 来连接条件，可以使用 UNION 进行连接【innodb表or不走索引，union会走索引】
	- 能用 union all 就不用 union，union 过滤重复数据要耗费更多的 CPU 资源
	- 避免部分 like 查询，如 '%ConstXiong%'
	- 避免在索引列上使用计算、函数
	- in 和 not in 慎用，能用 between 不要用 in
	- select 子句中避免使用 *
3. 单表数据量太大。解决办法：
	-  分页查询(在索引上完成排序分页操作、借助主键进行关联)
	-  单表数据过大，进行分库分表
	-  考虑使用非关系型数据库提高查询效率
	-  全文索引场景较多，考虑使用 ElasticSearch、solr

通过explain命令，我们能分析出一些慢SQL的常见原因：
-   索引使用问题，通过 `possible_keys(能用到的索引)` 和 `key(实际用到的索引)` 两个字段查看：
    -   没有使用索引
    -   优化器选择了错误索引
    -   没有实现覆盖索引
-   I/O开销问题，通过`rows(执行当前查询要遍历的行数)`和`filtered(有效行数/扫描行数比值)`字段来查看：
    -   扫描的行数过多
    -   返回无用列且无用列有明显I/O性能开销(比如text、blob、json 等类型）

### explain参数🧊
[参考](https://segmentfault.com/a/1190000020765817)
type
- system：表中只有一条数据，这是一个特殊的const 类型；
- const：针对主键或唯一索引的等值查询扫描，最多只返回一行数据，const 查询速度非常快，因为它仅仅读取一次即可；
- eq_ref：此类型通常出现在多表的 join 查询，表示对于前表的每一个结果,都只能匹配到后表的一行结果，并且查询的比较操作通常是＝, 查询效率较高；【唯一性索引扫描，对于索引字段的值，表中只有一条记录与之匹配，常见于主键和唯一性索引扫描】
- ref：此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询；【非唯一性索引扫描，可能返回多条数据】
- ref_or_null：与ref方法类似，只是增加了null值的比较。实际用的不多；
- index_merge：表示查询使用了两个以上的索引，最后取交集或者并集，常见and,or的条件使用了不同的索引。
- range：表示使用索引范围查询，通过索引字段范围获取表中部分数据记录。这个类型通常出现在 =, <>, >, >=, <, <=, IS NULL, <=>, BETWEEN, IN操作中，此时输出的 ref 字段为 NULL并且key_len字段是此次查询中使用到的索引的最长的那个；
- index：全表扫描，只是扫描表的时候按照索引次序进行而不是行。主要优点就是避免了排序，但是开销仍然非常大，这种情况时, Extra 字段会显示 Using index；【只遍历索引树，这通常要比All快，因为索引文件通常比数据文件小。index是从索引中查询，ALL是从磁盘。】
- all：性能最差的情况，使用了全表扫描，系统必须避免出现这种情况。

possible_keys：能用到的索引。
key：真正用到的索引。
key_len：使用了索引字节的长度。
rows：扫描了多少行数，也是性能评估的重要依据
extra：
- Distinct：一旦找到了与行相联合匹配的行就不再搜索了；
- Using filesort：使用了文件排序，性能非常慢，需要优化。
- Using index：查询使用到了索引，列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候。【覆盖索引】
- Using temporary：使用了临时表排序，性能非常慢，需要优化。
- Using where：表示使用了where进行查询，不是很重要。
- ALL：这个连接类型对于前面的每一个记录联合进行完全扫描，这一般比较糟糕，需要优化。

### EXPLAIN执行计划中要重点关注哪些要素
[参考](https://www.cnblogs.com/ajianbeyourself/p/6676401.html)
- type	本次查询表联接类型，从这里可以看到本次查询大概的效率
- key	最终选择的索引，如果没有索引的话，本次查询效率通常很差
- key_len	本次查询用于结果过滤的索引实际长度
- rows	预计需要扫描的记录数，预计需要扫描的记录数越小越好
- Extra	额外附加信息，主要确认是否出现 Using filesort、Using temporary 这两种情况

### explain索引扫描类型
[参考](https://cloud.tencent.com/developer/article/1672295)
- all：全表扫描
- index：将所有的索引树都遍历一遍，查找到符合条件的行。
- range：体现在对某个索引进行区间范围检索，一般出现在 where 条件中的 between、and、<、>、in 等范围查找中。
- ref：不是主键索引，也不是唯一索引，就是普通的索引，可能会返回多个符合条件的行。
- eq_ref：类似ref，区别在于使用的是唯一索引，使用主键的关联查询
- const：将主键索引或者唯一索引放到 where 条件中查询，MySQL 可以将查询条件转变成一个常量，只匹配一行数据，索引一次就找到数据了。
- system：属于const类型的特例,表只有一条记录行
- null：MySQL不访问任何表或索引，直接返回结果

由上至下，效率越来越高

###  filesort出现在哪种场景下？
当Where 条件和 order by 子句作用在不同的列上，建立联合索引可以避免Using filesort的产生。

### 建索引的注意事项🐋
[参考](https://www.cnblogs.com/TF511/articles/10931434.html)
- 经常与其他表进行连接的表，在连接字段上应该建立索引；
- 经常出现在Where子句中的字段，特别是大表的字段，应该建立索引；
- 索引应该建在选择性高的字段上；
- 索引应该建在小字段上，对于大的文本字段甚至超长字段可以建立**前缀索引**；
- 在经常需要搜索查询的列上创建索引，可以加快搜索的速度；
-  对复合索引，按照字段在查询条件中出现的频度建立索引。只有复合索引的第一个字段出现在查询条件中，该索引才可能被使用。
-  为经常出现在关键字order by、group by、distinct后面的字段，建立索引。
- 限制表上的索引数目。对一个存在大量更新操作的表，所建索引的数目一般不要超过3个，最多不要超过5个。索引虽说提高了访问速度，但太多索引会影响数据的更新操作。
- 删除不再使用，或者很少被使用的索引。

[abc索引的使用情况](https://www.jianshu.com/p/499cf5795de5)

### 分页查询优化
offset，limit
```sql
select * from orders_history where type=8 limit 100,100;
select * from orders_history where type=8 limit 1000,100;
select * from orders_history where type=8 limit 10000,100;
select * from orders_history where type=8 limit 100000,100;
select * from orders_history where type=8 limit 1000000,100;
```
随着查询偏移的增大，尤其查询偏移大于10万以后，查询时间急剧增加。
这种分页查询方式会从数据库第一条记录开始扫描，所以越往后，查询速度越慢，而且查询的数据越多，也会拖慢总查询速度。

比较差的limit的情况：
- 查询到索引叶子节点数据。
- 根据叶子节点上的主键值去聚簇索引上查询需要的全部字段值。

优化方法：[参考](https://segmentfault.com/a/1190000038704015)

使用子查询优化
先定位偏移位置的 id，然后往后查询，这种方式适用于 id 递增的情况。
```mysql
select * from orders_history where type=8 and 
id>=(select id from orders_history where type=8 limit 100000,1) 
limit 100;
```

### 分库分表
[参考](https://zhuanlan.zhihu.com/p/136963357)
[参考二](https://www.shouxicto.com/article/2941.html)
- 垂直分表：表中的字段较多，一般将不常用的、 数据较大、长度较长的拆分到“扩展表“。
- 水平分表：单表的数据量太大。按照某种规则（RANGE,HASH取模等），切分到多张表里面去。 但是这些表还是在同一个库中，所以库级别的数据库操作还是有IO瓶颈。

- 垂直分库：一个数据库的表太多。此时就会按照一定业务逻辑进行垂直切，比如用户相关的表放在一个数据库里，订单相关的表放在一个数据库里。注意此时不同的数据库应该存放在不同的服务器上，此时磁盘空间、内存、TPS等等都会得到解决。
- 水平分库：它是指将单张表的数据切分到多个服务器上去，每个服务器具有相应的库与表，只是表中数据集合不同。

分库能够解决的问题是，数据库读写QPS过高，数据库连接数不足；【并发量高分库】
分表能够解决的问题是，单表数据量过大，查询、存储性能遇到瓶颈；【数据量大分表】

**分库分表的问题**
1. 联合查询困难
2. 需要支持分布式事务
3. 跨库join困难
4. 分布式ID

### [水平分库分表](https://cloud.tencent.com/developer/article/1893163)🌟🌟
[参考一](https://www.shouxicto.com/article/2941.html)
[参考二](https://zhuanlan.zhihu.com/p/136963357)
[最新的参考](https://zhuanlan.zhihu.com/p/99396275)
 1. 查询切分：将ID和库的Mapping关系记录在一个单独的库中。
 2. 范围切分：比如按照时间区间或ID区间来切分。
 3. Hash切分：数据水平切分后我们希望是一劳永逸或者是易于水平扩展的，所以推荐采用mod 2^n这种一致性Hash。
	 - 以统一订单库为例，我们分库分表的方案是32 * 32的，即通过UserId后四位mod 32分到32个库中，同时再将UserId后四位Div 32 Mod 32将每个库分为32个表，共计分为1024张表。
	 - 如果32个集群也无法满足需求，那么将分库分表规则调整为(32 * 2^n) * (32⁄2^n)，可以达到最多1024个集群。
	 - 分库分表目前默认使用的是取模算法，分表算法为 (#shard_key % (group_shard_num * table_shard_num))，分库算法为 (#shard_key % (group_shard_num * table_shard_num)) / table_shard_num，其中group_shard_num为分库个数，table_shard_num为每个库的分表个数。
	 - 例如把一张大表分成100张小表然后散到2个库，则0-49落在第一个库、50-99落在第二个库。

### [分库分表会遇到什么问题，怎么解决](https://www.jianshu.com/p/32b3e91aa22c)
1. 事务问题：分布式事务和通过应用程序与数据库共同控制实现事务
2. 跨节点Join的问题：分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。
3. 跨节点的count,order by,group by以及聚合函数问题：分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。
4. 数据迁移，容量规划，扩容等问题：利用对2的倍数取余具有向前兼容的特性（如对4取余得1的数对2取余也是1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。
5. ID问题：UUID、Snowflake

```mysql
REPLACE INTO sequence (stub) VALUES ('a');  
SELECT LAST_INSERT_ID();  
```

### 联合索引与多个单列索引的区别
[参考](https://www.cnblogs.com/liudaya/p/13226984.html)
所以说创建复合索引时，应该仔细考虑列的顺序。对索引中的所有列执行搜索或仅对前几列执行搜索时，复合索引非常有用；仅对后面的任意列执行搜索时，复合索引则没有用处。
多个单列索引在多条件查询时只会生效第一个索引！所以多条件联合查询时最好建联合索引！【and的情况】
在创建联合索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。
当创建(a,b,c)联合索引时，相当于创建了(a)单列索引，(a,b)联合索引以及(a,b,c)联合索引 
想要索引生效的话,只能使用 a和a,b和a,b,c三种组合；当然，我们上面测试过，a,c组合也可以，但实际上只用到了a的索引，c并没有用到！ 

有一个比较特殊的查询条件：where a = 1 and c = 3 ，符合最左匹配吗？
- MySQL 5.5 的话，前面 a 会走索引，在联合索引找到主键值后，开始回表，到主键索引读取数据行，然后再比对 c 字段的值。
- 从 MySQL5.6 之后，有一个索引下推功能，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。
- 比如下面这条 where a = 1 and c = 0 语句，我们可以从执行计划中的 Extra=Using index condition 使用了索引下推功能。

在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。这是因为 OR 的含义就是两个只要满足一个即可，因此只有一个条件列是索引列是没有意义的，只要有条件列不是索引列，就会进行全表扫描。如果改为and，可以看到 type=index merge， index merge 的意思就是对 id 和 age 分别进行了扫描，然后将这两个结果集进行了合并，这样做的好处就是避免了全表扫描。

[参考](https://bbs.huaweicloud.com/blogs/333163)

今天给大家介绍了 6 种会发生索引失效的情况：
-   当我们使用左或者左右模糊匹配的时候，也就是 `like %xx` 或者 `like %xx%`这两种方式都会造成索引失效；
-   当我们在查询条件中对索引列使用函数，就会导致索引失效。
-   当我们在查询条件中对索引列进行表达式计算，也是无法走索引的。
-   MySQL 在遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较。如果字符串是索引列，而条件语句中的输入参数是数字的话，那么索引列会发生隐式类型转换，由于隐式类型转换是通过 CAST 函数实现的，等同于对索引列使用了函数，所以就会导致索引失效。
-   联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。
-   在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。

### IO密集型优化
多并发又可以分为CPU密集型和IO密集型：
1. CPU密集型即需要非常多的CPU计算资源，如有多颗CPU核心，可以让每一颗CPU都参与计算，从而不浪费服务器资源，CPU密集型的典型例子，例如文件排序、图形搜索、动态规划等需要复杂的科学计算的情况，CPU密集型需要减少线程数，减少线程的上下文切换导致的资源损耗。
2. IO密集型即需要大量的数据读写，例如网络传输、数据库读写等，大部分的网站、企业应用系统都属于IO密集型，当发生IO读写的时候，由于IO操作时间很长（受限于硬盘的读写速度和网络传输速度），线程会处于等待状态，此时CPU空闲，这时CPU可以调度其他线程进行处理，IO密集型需要增加线程数，这样在IO处理的时候，可以去做其他事情，以提高并发量。

优化：
- 建立连接池：如果有空闲等待的线程，则将任务放入队列中，让线程去处理任务。如果当前线程数小于最大线程数，则返回 false ，让线程池去创建新的线程。否则，就将任务放入队列中。
- 使用消息队列
- 使用集群

### [关系型数据库和非关系型数据库的区别](https://www.cnblogs.com/xrq730/p/11039384.html)
- 关系型数据天然就是表格式的，因此存储在数据表的行和列中。数据表可以彼此关联协作存储，也很容易提取数据。
- 非关系型数据不适合存储在数据表的行和列中，而是大块组合在一起。非关系型数据通常存储在数据集中，就像文档、键值对或者图结构。

### [MySQL怎么实现join](https://blog.csdn.net/youyou1543724847/article/details/106916369)

### 增删改比较多的情况下，为什么一段时间后要重新建立索引?
**索引出现碎片**
Oracle建议对于索引深度超过4级以及已删除的索引条目至少占有现有索引条目总数的20%，这2种情形下需要重建索引。
**索引虚高**
如果这时候的索引关键字是一个不断增大的id，那么被标记为删除的索引条目就永远不会被重用，那树就不会不断增长，也就出现了，表的数据空间减少了，而索引的数据空间却在不断增大的情况。由于索引的高度不断增加，访问索引数据时需要访问更多的索引块。

### 关系型数据库的三大范式
- 第一范式：所有字段值都是不可分解的
- 第二范式：确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）
- 第三范式：确保数据表中的每一列数据都和主键直接相关，而不能间接相关。
***
第一范式强调每一列都是不可分割的原子数据项。

第二范式在1NF的基础上，非属性码的属性必须完全依赖于主码。（在1NF基础上消除非属性码的属性对主码的部分函数依赖）
学生基本信息表R中（学号，身份证号，姓名）当然学号属性取值是唯一的，在R关系中，（学号，身份证号）->（姓名），（学号）->（姓名），（身份证号）->（姓名）；所以姓名部分函数依赖与（学号，身份证号）；【表中其他数据元素都依赖于主关键字】

第三范式在2NF基础上，消除传递依赖。

### where和having的区别🐋
Where 是一个约束声明，使用Where约束来自数据库的数据，Where是在**结果返回之前**起作用的，Where中不能使用聚合函数。

Having是一个过滤声明，是在查询**返回结果集以后**对查询结果进行的过滤操作，在Having中可以使用聚合函数。

在查询过程中聚合语句(sum,min,max,avg,count)要比having子句优先执行。而where子句在查询过程中执行优先级高于聚合语句。

### 从三张表中查询平均分数大于85的学生

### 事务的两阶段提交🐋
[参考](https://cloud.tencent.com/developer/article/1790507)
**两阶段提交步骤**
1. 写入redo log，处于prepare状态
2. 写binlog
3. 修改redo log状态为commit

**为什么需要两阶段提交？**
- 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，与原库的值不同。
- 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效。之后用 binlog 来恢复的时候就多了一个事务出来，与原库的值不同。

**崩溃恢复**
- redo log 处于 **prepare 阶段之后、写 binlog 之前**，发生了崩溃（crash），由于此时 binlog 还没写，redo log 也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog 还没写，所以也不会传到备库。
- 如果 redo log 里面的事务是完整的，也就是已经有了 **commit 标识**，则直接提交；
- 如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整：
	a. 如果是，则提交事务；
	b. 否则，回滚事务。

### binlog和redo log的一致性问题
[参考](https://jishuin.proginn.com/p/763bfbd70f06)

![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/redolog一致性.png)

据两阶段提交，崩溃恢复时的判断规则是这样的：
- 如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交
- 如果 redo log 里面的事务处于 prepare 状态，则判断对应的事务 binlog 是否存在并完整
	a. 如果 binlog 存在并完整，则提交事务；
	b. 否则，回滚事务。

MySQL 咋知道 bin log 是不是完整的？
-   statement 格式的 bin log，最后会有 COMMIT
-   row 格式的 bin log，最后会有 XID event

### LOG
redo日志: 
现代数据库都需要写redo日志，例如修改一条数据，首先写redo日志，然后再写数据。在写完redo日志后，就直接给客户端返回成功。这样虽然看过去多写了一次盘，但是由于把对磁盘的随机写入(写数据)转换成了顺序的写入(写redo日志)，性能有很大幅度的提高。当数据库挂了之后，通过扫描redo日志，就能找出那些没有刷盘的数据页(在崩溃之前可能数据页仅仅在内存中修改了，但是还没来得及写盘)，保证数据不丢。

undo日志: 
数据库还提供类似撤销的功能，当你发现修改错一些数据时，可以使用rollback指令回滚之前的操作。这个功能需要undo日志来支持。此外，现代的关系型数据库为了提高并发(同一条记录，不同线程的读取不冲突，读写和写读不冲突，只有同时写才冲突)，都实现了类似MVCC的机制，在InnoDB中，这个也依赖undo日志。为了实现统一的管理，与redo日志不同，undo日志在Buffer Pool中有对应的数据页，与普通的数据页一起管理，依据LRU规则也会被淘汰出内存，后续再从磁盘读取。与普通的数据页一样，对undo页的修改，也需要先写redo日志。

检查点: 
英文名为checkpoint。数据库为了提高性能，数据页在内存修改后并不是每次都会刷到磁盘上。checkpoint之前的数据页保证一定落盘了，这样之前的日志就没有用了(由于InnoDB redolog日志循环使用，这时这部分日志就可以被覆盖)，checkpoint之后的数据页有可能落盘，也有可能没有落盘，所以checkpoint之后的日志在崩溃恢复的时候还是需要被使用的。InnoDB会依据脏页的刷新情况，定期推进checkpoint，从而减少数据库崩溃恢复的时间。检查点的信息在第一个日志文件的头部。

### 崩溃恢复
[参考](https://blog.51cto.com/u_13064681/1948187)
前滚数据库，主要分为两阶段，首先是日志扫描阶段，扫描阶段按照数据页的space_id和page_no分发redo日志到hash_table中，保证同一个数据页的日志被分发到同一个哈希桶中，且按照lsn大小从小到大排序。扫描完后，再遍历整个哈希表，依次应用每个数据页的日志，应用完后，在数据页的状态上至少恢复到了崩溃之前的状态。

执行完了redo前滚数据库，数据库的所有数据页已经处于一致的状态，undo回滚数据库就可以安全的执行了。数据库崩溃的时候可能有一些没有提交的事务或者已经提交的事务，这个时候就需要决定是否提交。主要分为三步，首先是扫描undo日志，重新建立起undo日志链表，接着是，依据上一步建立起的链表，重建崩溃前的事务，即恢复当时事务的状态。最后，就是依据事务的不同状态，进行回滚或者提交。

如果事务是TRX_STATE_PREPARED状态，那么在InnoDB层，不做处理，需要在Server层依据binlog的情况来决定是否回滚事务，如果binlog已经写了，事务就提交，因为binlog写了就可能被传到备库，如果主库回滚会导致主备数据不一致，如果binlog没有写，就回滚事务。

### LOG
[写的挺好](https://cloud.tencent.com/developer/article/1801920)
 
 innodb_flush_log_at_trx_commit

![](image/事务刷新机制.png)

重启innodb时，首先会检查磁盘中数据页的LSN，如果数据页的LSN小于日志中的LSN，则会从checkpoint开始恢复。

### 回表
如果 select 所需获得列中有大量的非索引列，索引就需要到表中找到相应的列的信息，这就叫回表。

举例，在使用idx_name_birthday_phone_number索引进行查询时大致可以分为这两个步骤：
- 从索引idx_name_birthday_phone_number对应的B+树中取出name值在Asa～Barlow之间的用户记录。
- 由于索引idx_name_birthday_phone_number对应的B+树用户记录中只包含name、birthday、phone_number、id这4个字段，而查询列表是*，意味着要查询表中所有字段，也就是还要包括country字段。这时需要把从上一步中获取到的每一条记录的id字段都到聚簇索引对应的B+树中找到完整的用户记录，也就是我们通常所说的回表，然后把完整的用户记录返回给查询用户。

查询优化器做的工作，查询优化器会事先对表中的记录计算一些统计数据，然后再利用这些统计数据根据查询的条件来计算一下需要回表的记录数，需要回表的记录数越多，就越倾向于使用全表扫描，反之倾向于使用二级索引 + 回表的方式。

覆盖索引：只需要在一棵索引树上就能获取SQL所需的所有列数据，无需回表，速度更快。

索引下推：MySQL 5.6引入了索引下推优化，索引下推可以在索引遍历过程中，对索引中包含的所有字段先做判断，过滤掉不符合条件的记录之后再回表，可以有效的减少回表次数。

### Innodb 页目录
B+树索引是InnoDB数据页的主要组成部分。各个数据页可以组成一个双向链表，而每个数据页中的记录会按照主键值从小到大的顺序组成一个单向链表，每个数据页都会为存储在它里边儿的记录生成一个页目录。再通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽。
![](https://github.com/qingzhu0214/JavaPage/raw/wuzu/_posts/myimg/页目录.png)

### MySQL里面的乐观锁和悲观锁熟悉吗？
[参考](https://segmentfault.com/a/1190000022839728)
要使用悲观锁，我们必须关闭mysql数据库的自动提交属性，因为MySQL默认使用autocommit模式，也就是说，当你执行一个更新操作后，MySQL会立刻将结果进行提交。

与普通查询不一样的是，我们使用了select…for update的方式，这样就通过数据库实现了悲观锁。此时在t_goods表中，id为1的 那条数据就被我们锁定了，其它的事务必须等本次事务提交之后才能执行。这样我们可以保证当前的数据不会被其它事务修改。

select ... 和select…for update不冲突

MySQL  InnoDB默认Row-Level Lock，所以只有「明确」地指定主键或者有索引，MySQL 才会执行Row lock (只锁住被选取的数据)  ，否则MySQL 将会执行Table Lock (将整个数据表单给锁住)。

### 乐观锁和悲观锁🐋
共享锁：
在查询语句后面增加LOCK IN SHARE MODE
排他锁：
在查询语句后面增加FOR UPDATE

[参考](https://blog.csdn.net/She_lock/article/details/82022431)
mysql InnoDB引擎默认的修改数据语句，update,delete,insert 都会自动给涉及到的数据加上排他锁，select 语句默认不会加任何锁类型。所以加过排他锁的数据行在其他事务种是不能修改数据的，也不能通过for update和lock in share mode锁的方式查询数据，但可以直接通过select ...from...查询数据，因为普通查询没有任何锁机制。因为没用上锁机制不与排他锁互斥，但查到的数据是修改数据之前的老数据。

[参考](https://juejin.cn/post/6878884451162521613)
不要把他们和数据中提供的锁机制（行锁、表锁、排他锁、共享锁）混为一谈。其实，在DBMS中，**悲观锁正是利用数据库本身提供的锁机制来实现的。**

悲观锁：
如果一个事务执行的操作对某行数据应用了锁，那只有当这个事务把锁释放，其他事务才能够执行与该锁冲突的操作。悲观并发控制主要用于数据争用激烈的环境。

乐观锁（ Optimistic Locking ） 相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。一般的实现乐观锁的方式就是记录数据版本。

### 间隙锁能完全避免幻读么？
[参考](https://www.cnblogs.com/CoderAyu/p/11525408.html)
1. a事务先select，b事务insert确实会加一个gap锁，但是如果b事务commit，这个gap锁就会释放（释放后a事务可以随意操作），
2. a事务再select出来的结果在MVCC下还和第一次select一样，
3. 接着a事务不加条件地update，这个update会作用在所有行上（包括b事务新加的）
4. a事务再次select就会出现b事务中的新行，并且这个新行已经被update修改了.

### SQL执行的过程
[参考](https://blog.csdn.net/weter_drop/article/details/93386581)
[参考](https://blog.csdn.net/qq_42979842/article/details/100082340)
- SQL执行时，会通过**连接器**建立连接、获取权限；连接器会维持和管理连接。
- 然后，MySQL会通过**分析器**对SQL语句进行解析，分析语句各部分含义，然后按照语法规则判断SQL是否符合MySQL的语法。
- 经过分析器分析后，MySQL会对SQL请求进行**优化器**的处理，优化器对语句索引、连接顺序等情况判断，决定使用哪种执行方案最合适。
- 最后，就到了**执行器**的阶段，执行器根据表的引擎定义，去调用引擎接口，执行SQL语句。

***
查询语句：
- 先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在 MySQL8.0 版本以前，会先查询缓存，以这条 sql 语句为 key 在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。
- 通过分析器进行词法分析，提取 sql 语句的关键元素，如果是select，提取需要查询的表名,需要查询所有的列，和查询条件。然后判断这个 sql 语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。
- 接下来就是优化器进行确定执行方案，优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了。
- 进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。

更新语句
更新的时候需要记录日志，MySQL 自带的日志模块式 binlog，所有的存储引擎都可以使用，我们常用的 InnoDB 引擎还自带了一个日志模块 redo log。
- 根据条件查询出那一条语句
- 然后拿到查询的语句，作出修改，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。
- 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。
- 更新完成。

redo log 是 InnoDB 引擎特有的，其他存储引擎都没有，这就导致会没有 crash-safe 的能力(crash-safe 的能力即使数据库发生异常重启，之前提交的记录都不会丢失)，binlog 日志只能用来归档。

设 redo log 处于预提交状态，binglog 也已经写完了，这个时候发生了异常重启会怎么样呢？ 这个就要依赖于 MySQL 的处理机制了，MySQL 的处理过程如下：
- 判断 redo log 是否完整，如果判断是完整的，就立即提交。
- 如果 redo log 只是预提交但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 redo log, 不完整就回滚事务。

### MySQL其他存储引擎🐨
[参考](https://segmentfault.com/a/1190000012588602)
[参考2](https://zhuanlan.zhihu.com/p/50564425)
InnoDB： 支持事务处理，支持外键，支持崩溃修复能力和并发控制。如果需要对事务的完整性要求比较高（比如银行），要求实现并发控制（比如售票），那选择InnoDB有很大的优势。如果需要频繁的更新、删除操作的数据库，也可以选择InnoDB，因为支持事务的提交（commit）和回滚（rollback）。

MyISAM： MyISAM的优势在于占用空间小，处理速度快。缺点是不支持事务的完整性和并发性。如果在数据库中创建在整个生命周期内只读的表，则应该使用MyISAM的压缩型表来减少空间的占用。

MEMORY： 所有的数据都存储在内存中。默认使用哈希（HASH）索引，其速度比使用B-+Tree型要快。文件数据都存储在内存中，如果mysqld进程发生异常，重启或关闭机器这些数据都会消失。

### B与B+树的区别？🐋
1. B+树改进了B树, 让内结点只作索引使用, 去掉了其中指向data record的指针, 使得每个结点中能够存放更多的key, 因此能有更大的出度。 这样就意味着存放同样多的key, 树的层高能进一步被压缩, 使得检索的时间更短。
2. 由于底部的叶子结点是链表形式, 因此也可以实现更方便的顺序遍历。

### varchar和char的的区别是什么
[参考](https://cloud.tencent.com/developer/article/1793497)
- char的长度是不可变的，而varchar的长度是可变的。
- char的存取速度还是要比varchar要快得多，因为其长度固定，方便程序的存储与查找。
- char的存储方式是：对英文字符（ASCII）占用1个字节，对一个汉字占用两个字节。varchar的存储方式是：对每个英文字符占用2个字节，汉字也占用2个字节。

### MySQL常用的函数
LENGTH、CONCAT、REPLACE、TRIM、SUBSTRING、REVERSE
MAX、MIN、COUNT、SUM、AVERAGE

### 一个数据庞大(5000W)的数据库，你会怎么做优化，尽快的查询出来？
[参考](https://www.zhihu.com/question/19719997)
第一优化你的sql和索引；
第二加缓存，memcached,redis；
第三以上都做了后，还是慢，就做主从复制或主主复制，读写分离；
垂直拆分，水平拆分

### 关系型数据库和非关系型区别
[参考](https://www.huaweicloud.com/zhishi/1592288147096.html)
1. 成本:Nosql数据库很容易部署，基本上是开源软件，无需像Oracle那样花费大量成本购买，比关系数据库便宜。
2. 查询速度:Nosql数据库将数据存储在高速缓存中，不需要对SQL层进行分析。关系数据库在硬盘上存储数据，自然的查询速度远比Nosql数据库慢。
3. 存储数据的格式：Nosql的存储格式是key,value形式、文档形式、图片形式等等，所以可以存储基础类型以及对象或者是集合等各种格式，而数据库则只支持基础类型。
4. 扩展性：关系型数据库有类似join这样的多表查询机制的限制导致扩展很艰难。Nosql基于键值对，数据之间没有耦合性，所以非常容易水平扩展。
5. 持久存储：Nosql不使用于持久存储，海量数据的持久存储，还是需要关系型数据库
6. 数据一致性：非关系数据库通常强调数据的最终一致性，而不是像关系数据库那样强烈的数据一致性，以及从非关系数据库读取的数据可能仍处于中间状态，Nosql不提供对事务的处理。

### 数据库代码里面如何处理死锁
锁超时就是一个事务 A 需要的资源正在被别的事务 B 占有，假如数据库设置的超时时间为 60 秒，超过了 60 秒，事务 B 仍没有释放资源，那么事务 A 将报锁超时错误并回滚。

首先要定位出导致锁等待或锁超时的事务，db2 提供了锁定事件监控器以捕获其锁定数据的活动。 

### 隔离级别与锁的关系
- 在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突
- 在Read Committed级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁；
- 在Repeatable Read级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。
- SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。

### InnoDB怎么刷库
[参考](https://www.cnblogs.com/frankcui/p/15227775.html)
- MySQL Server 层的执行器调用 InnoDB 存储引擎的数据更新接口；
- 存储引擎更新 Buffer Pool 中的缓存页，
- 同时存储引擎记录一条 redo log 到 redo log buffer 中，并将该条 redo log 的状态标记为 prepare 状态；
- 接着存储引擎告诉执行器，可以提交事务了。执行器接到通知后，会写 binlog 日志，然后提交事务；
- 存储引擎接到提交事务的通知后，将 redo log 的日志状态标记为 commit 状态；
- 接着根据 innodb_flush_log_at_commit 参数的配置，决定是否将 redo log buffer 中的日志刷入到磁盘。

### buffer pool宕机怎么办
 Buffer Pool 中所有缓存页都是处于内存当中的，当 MySQL 宕机或者机器断电，内存中的数据就会丢失，因此 MySQL 为了防止缓存页中的数据在更新后出现数据丢失的现象，引入了 redo log 机制。
 
当进行增删改操作时，MySQL 会在更新 Buffer Pool 中的缓存页数据时，会记录一条对应操作的 redo log 日志【Write-Ahead Log（WAL）】，这样如果出现 MySQL 宕机或者断电时，如果有缓存页的数据还没来得及刷入磁盘，那么当 MySQL 重新启动时，可以根据 redo log 日志文件，进行数据重做，将数据恢复到宕机或者断电前的状态，保证了更新的数据不丢失，因此 redo log 又叫做重做日志。它的本质是保证事务提交后，更新的数据不丢失。

我们写入的 redo log 日志，最终实际上是先写入在 redo log buffer 的 redo log block 中，然后在某一个**合适的时间点**，将这条 redo log 所在的 redo log block 刷入到磁盘中。

- MySQL 正常关闭的时候；
- MySQL 的后台线程每隔一段时间定时的讲 redo log buffer 刷入到磁盘，默认是每隔 1s 刷一次；
- 当 redo log buffer 中的日志写入量超过 redo log buffer 内存的一半时，即超过 8MB 时，会触发 redo log buffer 的刷盘；
- 当事务提交时，根据配置的参数 innodb_flush_log_at_trx_commit 来决定是否刷盘。如果 innodb_flush_log_at_trx_commit 参数配置为 0，表示事务提交时，不进行 redo log buffer 的刷盘操作；如果配置为 1，表示事务提交时，会将此时事务所对应的 redo log 所在的 redo log block 从内存写入到磁盘，同时调用 fsync，**确保数据落入到磁盘**；如果配置为 2，表示只是将日志写入到操作系统的缓存，而不进行 fsync 操作。（进程在向磁盘写入数据时，是先将数据写入到操作系统的缓存中：os cache，再调用 fsync 方法，才会将数据从 os cache 中刷新到磁盘上）

写 redo log 时，我们将 redo log 日志追加到文件末尾，虽然也是一次磁盘 IO，但是这是顺序写操作（不需要移动磁头）；而对于直接将数据更新到磁盘，涉及到的操作是将 buffer pool 中缓存页写入到磁盘上的数据页上，由于涉及到寻找数据页在磁盘的哪个地方，这个操作发生的是随机写操作（需要移动磁头），相比于顺序写操作，磁盘的随机写操作性能消耗更大，花费的时间更长，因此 redo log 机制更优，能提升 MySQL 的性能。

### InnoDB RR级别解决幻读了吗
[幻读写的很好](https://gaoooyh.github.io/2021-09-28-MySQL-InnoDB-RR(%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB)%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E8%83%BD%E5%90%A6%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB)
- 如果前后两次select 都加锁, 那么它和 serializable 隔离级别一样, 因为mysql 会通过gap lock & next key lock 防止其他事务在where 范围内插入/删除。
- 如果前一次不加锁, 后一次加锁, 那么它和 read committed 隔离级别一样, 因为后一次通过加锁(当前读), 读取到的一定是最新一条已提交事务的数据。
- 如果前后两次select 都不加锁, 则基于MySQL的快照读机制, 创建read-view, 并保证后一次读和前一次读数据一致. 但是如果两次select之间 有update, update 是只有当前读的, 当前读不受read-view限制, 它会去竞争要修改的数据行的锁, 进而可能会对 trx_id 处于read-view中的数据行(快照读不可见) 进行了修改, 数据行的trx_id也被修改为当前事务的trx_id, 然后再使用普通select时就变得可见了。

在只读时是可以避免幻读的，在读写时可能会因为update操作使得不可见的行变得可见, 从而出现幻影行。

### MySql为何默认不用hash索引
[参考](https://blog.csdn.net/hw20070575/article/details/51248321)
- Hash 索引仅仅能满足"=","IN"和"<=>"查询，不能使用范围查询。
- Hash 索引无法被用来避免数据的排序操作。
- Hash 索引不能利用部分索引键查询。
- Hash 索引在任何时候都不能避免表扫描。【存在哈希冲突，要通过访问表中的实际数据进行相应的比较，并得到相应的结果。】
- Hash 索引遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高。

### 事务id
[参考](https://wwwxz.blog.csdn.net/article/details/121226589)
只有在事务**对表中的记录进行改动**时才会为这个事务分配一个唯一的事务id，否则事务id值默认为0。

事务id本质上就是一个数字，事务id生成策略如下：
- 内存中维护一个全局变量，每当需要为某个事务分配事务id时，就会把该变量值当作事务id分配给该事务，并且自增1。
- 每当这个变量的值为256的倍数时，就会将值刷新到系统表空间中页号为5的页面中一个名为Max Trx ID的属性中（占用8个字节）。
- 当系统下一次启动时，会将Max Trx ID的值加载到到内存中，并加上256之后赋值给前面提到的全局变量。

### Hash Join怎么实现
[参考](https://zhuanlan.zhihu.com/p/121301503)

Join有三种算法，分别是Nested Loop Join，Hash Join，Sort Merge Join。

- 选择一个表（一般情况下是较小的那个表，以减少建立哈希表的时间和空间），对其中每个元组上的连接属性（join attribute)采用哈希函数得到哈希值，从而建立一个哈希表。
- 对另一个表，扫描它的每一行并计算连接属性的哈希值，与bulid phase建立的哈希表对比，若有落在同一个bucket的，如果满足连接谓词（predicate)则连接成新的表。

Grace hash join：这个方法适合用于内存不足的情况，核心在于分块处理。
- 第一阶段分块阶段(Partition Phase)：把每个关系（relation）分别用同一个哈希函数h(x)在连接属性上进行分块（partition）。分块后每个元组分配到对应的bucket，然后分别把这些buckets写到磁盘当中。
- 第二阶段和普通的哈希连接类似，将分别来自于两个关系对应的bucket加载到内存中，为较小的那个bucket构建哈希表（注意，这里一定要用**不同的哈希函数**，因为数据很多的情况下不同值的哈希值可能相同，但不同值的两个哈希值都相同可能性非常小）

### Nested Join怎么实现的
[参考](https://zhuanlan.zhihu.com/p/81398139)
mysql只支持一种join算法：Nested-Loop Join（嵌套循环连接），但Nested-Loop Join有三种变种：
- Simple Nested-Loop Join，简单嵌套
```java
// 伪代码
for (r in R) {
	for (s in S) {
		if (r satisfy condition s) {
			output <r, s>;
		}
	}
}
```
- Index Nested-Loop Join，索引嵌套：INLJ是在SNLJ的基础上做了优化，通过连接条件确定可用的索引，在Inner Loop中**扫描索引而不去扫描数据本身**，从而提高Inner Loop的效率。而INLJ也有缺点，就是如果扫描的索引是**非聚簇索引，并且需要访问非索引的数据**，会产生一个回表读取数据的操作，这就多了一次随机的I/O操作。
- Block Nested-Loop Join ，join buffer缓冲区嵌套（临时表）：BNLJ在SNLJ的基础上使用了join buffer，会提前读取Inner Loop所需要的记录到buffer中，以提高Inner Loop的效率。

在MySQL5.6中，对**INLJ的回表**操作进行了优化，增加了Batched Key Access Join（批量索引访问的表关联方式）和Multi Range Read（MRR，多范围读取）特性，在join操作中缓存所需要的数据的rowid，再批量去获取其数据，把I/O从多次零散的操作优化为更少次数批量的操作，提高效率。

### 笛卡尔积
[参考](https://blog.csdn.net/u014682191/article/details/53009871)
-  两表直接笛卡尔积的结果数量是两表的数据量相乘
- 带where条件id相等的笛卡尔积和inner join结果相同，但是inner join效率快一点
- left join：TEST_A表的ID为空时拼接TEST_B表的内容为空，right join则相反
- full join：等于left join和right join的并集

### 可扩展Hash
[参考](https://blog.csdn.net/t18438605018/article/details/103157044)

### 两阶段锁
[参考](https://zhuanlan.zhihu.com/p/59535337)
在2PL协议下，每个transaction都会经过两个阶段：在第一个阶段里，transaction根据需要不断地获取锁，叫做 growing phase (expanding phase)；在第二个阶段里，transaction开始释放其持有的锁，根据2PL的规则，这个transaction不能再获得新的锁，所以它所持有的锁逐渐减少，叫做 shrinking phase (contracting phase)。

如果一个transaction释放了它所持有的任意一个锁，那它就再也不能获取任何锁。

### MySQL的锁
[参考](https://codeantenna.com/a/OsN9DWV0ti)
- MySQL InnoDB 行锁是通过给索引上的**索引项**加锁来实现的。
- Oracle 是通过在数据块中对相应**数据行**加锁来实现的。

MySQL InnoDB这种行锁实现特点意味着**只有通过索引条件检索数据，InnoDB才使用行级锁**，否则，InnoDB将使用表锁！
由于MySQL的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果是**使用相同的索引键，是会出现锁冲突的**。

[参考](https://icode.best/i/66753544441000)

为什么锁住唯一索引后，主键索引也会被锁住？
- 这个是由InnoDB索引的存储和检索方式决定的。辅助索引中存储的是二级索引和主键的ID，所以锁住辅助索引后，会根据主键ID找到对应的主键索引，也锁定之。
- 而通过主键索引检索数据加锁，则只会锁住主键索引。

为什么表没有索引，表里所有的记录都会被锁住？
当表上没有创建索引的时候，InnoDB会为每一行创建一个隐藏的主键作为聚集索引。这个隐藏的主键是一个6个字节的列，该列的值会随着数据的插入自增。
当不通过索引检索数据的时候，MySQL会使用全表扫描，此时所有行的索引都会被锁定，行锁升级为表锁。

### 一致性Hash中的数据倾斜
如果节点的数量很少，而hash环空间很大（一般是 0 ~ 2^32），直接进行一致性hash上去，大部分情况下节点在环上的位置会很不均匀，挤在某个很小的区域。最终对分布式缓存造成的影响就是，集群的每个实例上储存的缓存数据量不一致，会发生严重的数据倾斜。

如果每个节点在环上只有一个节点，那么可以想象，当某一集群从环中消失时，它原本所负责的任务将全部交由顺时针方向的下一个集群处理。例如，当group0退出时，它原本所负责的缓存将全部交给group1处理。这就意味着group1的访问压力会瞬间增大。

设想一下，如果group1因为压力过大而崩溃，那么更大的压力又会向group2压过去，最终服务压力就像滚雪球一样越滚越大，最终导致雪崩。

解决上述两个问题最好的办法就是扩展整个环上的节点数量，因此我们**引入了虚拟节点**的概念。一个实际节点将会映射多个虚拟节点，这样Hash环上的空间分割就会变得均匀。同时，引入虚拟节点还会使得节点在Hash环上的顺序随机化，这意味着当一个真实节点失效退出后，它原来所承载的压力将会均匀地分散到其他节点上去。

但是如果节点很少，同样容易出现倾斜，负载不均衡问题。所以一致性哈希算法，引入了**虚拟节点**，在整个环上，均衡增加若干个节点。比如a1，a2，b1，b2，c1，c2，a1和a2都是属于A节点的。通过让闭环上的节点增加，来平衡各个节点散列的值。

### 三级封锁协议
 - 一级封锁协议是：**事务T在修改数据R之前必须先对其加X锁，直到事务结束才释放**。事务结束包括正常结束（COMMIT）和非正常结束（ROLLBACK）。使用一级封锁协议可以解决**丢失修改**问题。在一级封锁协议中，如果仅仅是读数据不对其进行修改，是不需要加锁的，它不能保证可重复读和不读“脏”数据。
 -  二级封锁协议是：一级封锁协议加上事务T在读取数据R之前必须先对其加S锁，**读完后**方可释放S锁。二级封锁协议除防止了丢失修改，还可以进一步防止**读“脏”数据**。但在二级封锁协议中，由于读完数据后即可释放S锁，所以它不能保证可重复读。
 - 三级封锁协议是：一级封锁协议加上事务T在读取数据R之前必须先对其加S锁，直到**事务结束**才释放。三级封锁协议除防止了丢失修改和不读“脏”数据外，还进一步防止了不可重复读。

### SQL的执行过程
- 连接器： 主要负责用户登录数据库，进行用户的身份认证，包括校验账户密码，权限等操作
- 查询缓存: 主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集。
- 分析器: 第一步，词法分析，一条 SQL 语句有多个字符串组成，首先要提取关键字，比如 select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步语法分析，主要就是判断你输入的 SQL 是否正确，是否符合 MySQL 的语法。
- 优化器： 按照 MySQL 认为最优的方案去执行。
- 执行器: 首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果。

### 数据库出现崩溃情况如何恢复
当MySQL重启时会去找Checkpoint，并且根据Checkpoint的特性。MySQL可以明确的知道checkponit之前的脏数据已经落过盘了，重启时没必要进行重做。

在崩溃恢复中还需要回滚没有提交的事务，提交没有提交成功的事务。由于回滚操作需要undo日志的支持，undo日志的完整性和可靠性需要redo日志来保证，所以崩溃恢复先做redo前滚，然后做undo回滚。

### char和varchar区别
[参考](https://segmentfault.com/a/1190000024432795)
![](image/charvarchar.png)

### SQL 优化
exists和in的区别
[参考](https://www.51cto.com/article/689585.html)
- 避免使用select *
- 用union all代替union
- 小表驱动大表：如果sql语句中包含了in关键字，则它会优先执行in里面的子查询语句，然后再执行in外面的语句。如果in里面的数据量很少，作为条件查询速度更快。如果sql语句中包含了exists关键字，它优先执行exists左边的语句(即主查询语句)。然后把它作为条件，去跟右边的语句匹配。如果匹配上，则可以查询出数据。如果匹配不上，数据就被过滤掉了。
- 批量操作
- 多用limit
- 分页优化
- 用连接查询【join】代替子查询：mysql执**行子查询时，需要创建临时表**，查询完毕后，需要再删除这些临时表，有一些额外的性能消耗。这时可以改成连接查询。
- join的表不宜过多
- 在用left join关联查询时，左边要用小表，右边可以用大表。如果能用inner join的地方，尽量少用left join。【两张表使用left join关联，mysql会默认用left join关键字左边的表，去驱动它右边的表。】


### Join原理
[参考](https://blog.csdn.net/agonie201218/article/details/106993948)
MySQL内部采用了一种叫做 nested loop join（嵌套循环连接）的算法。Nested Loop Join 实际上就是通过驱动表的结果集作为循环基础数据，然后一条一条的通过该结果集中的数据作为过滤条件到下一个表中查询数据，然后合并结果。如果还有第三个参与 Join，则再通过前两个表的 Join 结果集作为循环基础数据，再一次通过循环查询条件到第三个表中查询数据，如此往复，基本上MySQL采用的是最容易理解的算法来实现join。所以驱动表的选择非常重要，驱动表的数据小可以显著降低扫描的行数。

Index Nested-LoopJoin（减少内层表数据的匹配次数）
索引嵌套循环连接是基于索引进行连接的算法，索引是基于内层表的，通过外层表匹配条件直接与内层表索引进行匹配，避免和内层表的每条记录进行比较， 从而利用索引的查询减少了对内层表的匹配次数，优势极大的提升了 join的性能。

>
原来的匹配次数 = 外层表行数 * 内层表行数
优化后的匹配次数= 外层表的行数 * 内层表索引的高度

Block Nested-Loop Join（减少内层表数据的循环次数）
缓存块嵌套循环连接通过一次性缓存多条数据，把参与查询的列缓存到Join Buffer 里，然后拿join buffer里的数据批量与**内层表**的数据进行匹配，从而减少了内层循环的次数（遍历一次内层表就可以批量匹配一次**Join Buffer里面的外层表**数据）。

优化
- 用小结果集驱动大结果集，减少外层循环的数据量：
- 如果小结果集和大结果集连接的列都是索引列，mysql在内连接时也会选择用小结果集驱动大结果集，因为索引查询的成本是比较固定的，这时候外层的循环越少，join的速度便越快。
- 为**匹配的条件**增加索引：争取使用INLJ，减少内层表的循环次数
- 增大join buffer size的大小：当使用BNLJ时，一次缓存的数据越多，那么外层表循环的次数就越少
- 减少不必要的字段查询：
	1. 当用到BNLJ时，字段越少，join buffer 所缓存的数据就越多，外层表的循环次数就越少；
	2. 当用到INLJ时，如果可以不回表查询，即利用到覆盖索引，则可能可以提示速度。

### 数据库left-join中on和where条件区别
[参考](https://www.runoob.com/w3cnote/sql-join-the-different-of-on-and-where.html)

关键原因就是 left join、right join、full join 的特殊性，不管 on 上的条件是否为真都会返回 left 或 right 表中的记录，full 则具有 left 和 right 的特性的并集。 而 inner jion 没这个特殊性，则条件放在 on 中和 where 中，返回的结果集是相同的。【即使on上某一列为空，也会返回null，然后再用where条件过滤】

### MySQL延迟关联
在做分页时会用到Limit关键字去筛选所需数据，limit接受1个或者2个参数，接受两个参数时第一个参数表示偏移量，即从哪一行开始取数据，第二个参数表示要取的行数。 如果只有一个参数，相当于偏移量为0。
当偏移量很大时，如limit 100000,10 取第100001-100010条记录，mysql会取出100010条记录然后将前100000条记录丢弃，这无疑是一种巨大的性能浪费。
![](image/延迟关联1.png)
![](image/延迟关联.png)

### 主备切换
[参考](https://juejin.cn/post/6955405748461371422)

### 什么时候用单索引，什么时候用联合索引？
单列索引的回表次数比联合索引多

### MySQL next-key lock 加锁范围是什么？
[参考](https://segmentfault.com/a/1190000040129107)
```mysql
// 查看锁信息
select * from performance_schema.data_locks;
```
- INDEX_NAME：锁定索引的名称
- LOCK_TYPE：锁的类型，对于 InnoDB，允许的值为 RECORD 行级锁 和 TABLE 表级锁。
- LOCK_MODE：锁的类型：S, X, IS, IX, and gap locks
- LOCK_DATA：锁关联的数据，对于 InnoDB，当 LOCK_TYPE 是 RECORD（行锁），则显示值。当锁在主键索引上时，则值是锁定记录的主键值。当锁是在辅助索引上时，则显示辅助索引的值，并附加上主键值。

![](image/间隙锁.png)
- id = 11 是肯定不存在的。但是加了 for update，这时需要加 next-key lock，id = 11 所属区间为 (10,15] 的左开右闭区间；
- 因为是等值查询，不需要锁 id = 15 那条记录，next-key lock 会退化为间隙锁；
- 最终区间为 (10,15) 的左开右闭区间。

结论：
- 对主键等值加锁，且值存在时，会对表添加意向锁，同时会对主键索引添加行锁。
- 在数据不存在时，主键等值查询，会锁住该主键查询条件所在的间隙。

![](image/lock关系.png)

非主键唯一索引：
- 非主键**唯一索引**等值查询，数据存在，for update 是会在主键加锁的，而 in share mode 只有在走覆盖索引的情况下，会仅在自己索引上加锁；

普通索引：
普通索引等值查询，因为**不能确定唯一性**，所以即使定位到记录，也是会向后查询，直到查询到不为该值的记录，从而锁定该值的区间；

### 数据库设计需要注意什么
[参考](https://www.cnblogs.com/Jtianlin/p/10224094.html)
- 必须使用InnoDB存储引擎
- 新库默认使用utf8mb4字符集
- 数据表、数据字段必须加入中文注释
- 开发、测试、线上环境隔离
- 不在数据库做计算，cpu计算务必移至业务层
- 平衡范式与冗余，为提高效率可以牺牲范式设计，冗余数据【订单记录表，是个快照，不会修改】
- 表必须有主键，例如自增主键【主键尽量自增，不会造成频繁的分页。如果是订单之类的不要用那种能相减算出一天订单量的主键】
- 禁止使用外键，如果有外键完整性约束，需要应用程序控制【外键会导致表与表之间耦合，update与delete操作都会涉及相关联的表，十分影响sql 的性能，甚至会造成死锁。高并发情况下容易造成数据库性能，大数据高并发业务场景数据库使用以性能优先】
- 控制单表数据量，单表记录控制在**千万级**
- 必须把字段定义为**NOT NULL**并且提供默认值
- 单表索引建议控制在5个以内
- 禁止在**更新十分频繁、区分度不高**的属性上建立索引
- 建立**组合索引**，必须把区分度高的字段放在前面

### mysql 为什么要默认可重复读的隔离级别
[参考](https://dominicpoi.com/2019/06/16/MySQL-1/)
早阶段Mysql(5.1版本之前)的Binlog类型Statement是默认格式。在master上执行的顺序为先删后插！【一个长的事务做删除操作，一个短的事务做插入擦做】而此时binlog为STATEMENT格式，是基于事务记录，在事务未提交前，二进制日志先缓存，提交后再写入记录的,因此顺序为先插后删！

后面改用Row格式就可以了。

### MySQL执行过程
[参考](https://tech.meituan.com/2022/04/21/slow-query-optimized-advice-driven-by-cost-model.html)
```mysql
select * from INFORMATION_SCHEMA.OPTIMIZER_TRACE\G;
```
![](image/mysql执行.png)